<div align="center">

# ConceptCaps

### A Distilled Concept Dataset for Interpretability in Music Models

[![arXiv](https://img.shields.io/badge/arXiv-2601.14157-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2601.14157)
[![Dataset](https://img.shields.io/badge/ğŸ¤—%20Dataset-ConceptCaps-yellow.svg?style=flat-square)](https://huggingface.co/datasets/bsienkiewicz/ConceptCaps)
[![License](https://img.shields.io/badge/License-CC--BY--4.0-green.svg?style=flat-square)](https://creativecommons.org/licenses/by/4.0/)

<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>
<a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>

</div>

## Overview

This repository contains the source code, configurations, and data used to create the **ConceptCaps** dataset â€” a concept-based music captioning dataset designed for interpretability research in text-to-audio (TTA) generation systems.

ConceptCaps provides structured musical concept annotations alongside natural language captions, enabling fine-grained analysis of how TTA models represent and generate musical concepts.

### Key Features

- **23k music-caption-audio triplets** with explicit labels from a 200-attribute taxonomy
- **Four concept categories**: genre, mood, instruments, tempo
- **Separated semantic modeling from text generation**: VAE learns attribute co-occurrence, LLM generates descriptions
- **Validated through multiple metrics**: CLAP alignment, BERTScore, MAUVE, and TCAV analysis

## TL;DR

Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure. ConceptCaps addresses this by:

1. Using a **VAE** to learn plausible attribute co-occurrence patterns
2. **Fine-tuning an LLM** to convert attribute lists into professional descriptions
3. Synthesizing audio with **MusicGen**

This separation improves coherence and controllability over end-to-end approaches.

## Dataset

The dataset is available on Hugging Face: **[bsienkiewicz/ConceptCaps](https://huggingface.co/datasets/bsienkiewicz/ConceptCaps)**

### Configurations

| Configuration | Samples | Audio |
|--------------|---------|-------|
| `default` | 5,358 | âŒ |
| `25pct` | 1,339 | âŒ |
| `10pct` | 535 | âŒ |
| `audio` | 5,358 | âœ… |
| `25pct-audio` | 1,339 | âœ… |
| `10pct-audio` | 535 | âœ… |

### Quick Start

```python
from datasets import load_dataset

# Load captions only
dataset = load_dataset("bsienkiewicz/ConceptCaps", "default")

# Load with audio
dataset = load_dataset("bsienkiewicz/ConceptCaps", "audio")
```

## Project Structure

```
â”œâ”€â”€ config/                 # Hydra configuration files
â”‚   â”œâ”€â”€ callbacks/          # Training callbacks (checkpoints, early stopping, etc.)
â”‚   â”œâ”€â”€ data/               # Data module configurations
â”‚   â”‚   â”œâ”€â”€ caption/        # Caption dataset configs
â”‚   â”‚   â”œâ”€â”€ tta/            # Text-to-audio configs
â”‚   â”‚   â””â”€â”€ vae/            # VAE dataset configs
â”‚   â”œâ”€â”€ evaluation/         # Evaluation metric configurations
â”‚   â”œâ”€â”€ generation/         # Generation pipeline configs
â”‚   â”œâ”€â”€ logger/             # Logging configurations (W&B)
â”‚   â”œâ”€â”€ lora/               # LoRA fine-tuning configs
â”‚   â”œâ”€â”€ model/              # Model architecture configs
â”‚   â”œâ”€â”€ paths/              # Path configurations
â”‚   â”œâ”€â”€ preset/             # Preset configurations
â”‚   â”œâ”€â”€ prompt/             # LLM prompt templates
â”‚   â”œâ”€â”€ sweeps/             # Hyperparameter sweep configs
â”‚   â””â”€â”€ trainer/            # PyTorch Lightning trainer configs
â”‚
â”œâ”€â”€ data/                   # Datasets and intermediate data
â”‚   â”œâ”€â”€ concepts_to_tags.json           # Concept taxonomy mapping
â”‚   â”œâ”€â”€ musiccaps_tag_frequencies.csv   # Tag frequency analysis
â”‚   â”œâ”€â”€ evaluation_results/             # Evaluation outputs
â”‚   â”œâ”€â”€ generated_captions/             # Generated caption datasets
â”‚   â””â”€â”€ mtg_jamendo/                    # MTG-Jamendo data
â”‚
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ DATASET_CARD.md     # Dataset card for Hugging Face
â”‚   â”œâ”€â”€ experiments/        # Experiment documentation
â”‚   â””â”€â”€ assets/             # Documentation assets
â”‚
â”œâ”€â”€ models/                 # Trained model checkpoints
â”‚   â”œâ”€â”€ best-genre-classifier.ckpt      # Genre classifier for TCAV
â”‚   â””â”€â”€ vae_final.pth                   # Final VAE model
â”‚
â”œâ”€â”€ notebooks/              # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ 1. Taxonomy and dataset distillation.ipynb
â”‚   â”œâ”€â”€ 2. VAE aspect modeling.ipynb
â”‚   â”œâ”€â”€ 3. MusicCaps and VAE generated dataset comparison.ipynb
â”‚   â”œâ”€â”€ 4. Conditioned caption inference.ipynb
â”‚   â”œâ”€â”€ 5. Audio generation analysis.ipynb
â”‚   â”œâ”€â”€ 6. Create final datasets.ipynb
â”‚   â””â”€â”€ 7. TCAV for genre classification.ipynb
â”‚
â”œâ”€â”€ scripts/                # Executable scripts
â”‚   â”œâ”€â”€ caption/            # Caption generation scripts
â”‚   â”œâ”€â”€ helper/             # Utility scripts
â”‚   â”œâ”€â”€ tta/                # Text-to-audio scripts
â”‚   â””â”€â”€ vae/                # VAE training scripts
â”‚
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ caption/            # Caption generation module
â”‚   â”‚   â”œâ”€â”€ data.py         # Data loading and processing
â”‚   â”‚   â”œâ”€â”€ model.py        # Model definitions
â”‚   â”‚   â”œâ”€â”€ inference.py    # Inference pipeline
â”‚   â”‚   â”œâ”€â”€ evaluation.py   # Caption evaluation metrics
â”‚   â”‚   â””â”€â”€ lightning_*.py  # PyTorch Lightning components
â”‚   â”‚
â”‚   â”œâ”€â”€ vae/                # Variational Autoencoder module
â”‚   â”‚   â”œâ”€â”€ data.py         # VAE data processing
â”‚   â”‚   â”œâ”€â”€ model.py        # VAE architecture
â”‚   â”‚   â”œâ”€â”€ inference.py    # VAE sampling/generation
â”‚   â”‚   â”œâ”€â”€ evaluation.py   # VAE evaluation
â”‚   â”‚   â””â”€â”€ lightning_module.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tcav/               # TCAV analysis module
â”‚   â”‚   â”œâ”€â”€ model.py        # Classifier for TCAV
â”‚   â”‚   â””â”€â”€ tcav.py         # TCAV implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ tta/                # Text-to-audio module
â”‚   â”‚   â”œâ”€â”€ audio.py        # Audio processing utilities
â”‚   â”‚   â”œâ”€â”€ data.py         # TTA data handling
â”‚   â”‚   â””â”€â”€ evaluation.py   # Audio evaluation metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ data/               # Common data utilities
â”‚   â”œâ”€â”€ utils/              # Shared utilities
â”‚   â””â”€â”€ constants.py        # Project constants
â”‚
â”œâ”€â”€ outputs/                # Training outputs and experiments
â”œâ”€â”€ environment.yml         # Conda environment specification
â”œâ”€â”€ Makefile                # Common development commands
â””â”€â”€ mkdocs.yml              # Documentation configuration
```

## Installation

### Prerequisites

- Python 3.12
- CUDA-compatible GPU (recommended)
- Conda package manager

### Setup

```bash
# Clone the repository
git clone https://github.com/bsienkiewicz/music-gen-interpretability
cd music-gen-interpretability

# Create conda environment
conda env create -f environment.yml

# Activate environment
conda activate conceptcaps
```

## Usage

### VAE Training

Train the VAE for learning attribute co-occurrence patterns:

```bash
python -m src.scripts.vae_training
```

### Caption Generation

Generate captions from attribute lists using the fine-tuned LLM:

```bash
python -m src.scripts.caption_inference
```

### Caption Model Fine-tuning

Fine-tune the caption generation model:

```bash
python -m src.scripts.caption_fine_tuning
```

### Text-to-Audio Inference

Generate audio from captions using MusicGen:

```bash
python -m src.scripts.tta_inference
```

### Configuration Override

Override any parameter from command line:

```bash
python -m src.scripts.vae_training trainer.max_epochs=100 data.batch_size=64
```

## Notebooks

The repository includes Jupyter notebooks demonstrating each pipeline stage:

| Notebook | Description |
|----------|-------------|
| `1. Taxonomy and dataset distillation.ipynb` | Concept taxonomy creation and tag mapping |
| `2. VAE aspect modeling.ipynb` | VAE training and attribute sampling |
| `3. MusicCaps and VAE generated dataset comparison.ipynb` | Dataset quality analysis |
| `4. Conditioned caption inference.ipynb` | Caption generation from concepts |
| `5. Audio generation analysis.ipynb` | MusicGen audio synthesis |
| `6. Create final datasets.ipynb` | Final dataset preparation |
| `7. TCAV for genre classification.ipynb` | TCAV interpretability analysis |

## Evaluation Metrics

ConceptCaps is validated through:

- **Audio-Text Alignment**: CLAP scores
- **Linguistic Quality**: BERTScore, MAUVE
- **Interpretability**: TCAV analysis confirming concept probes recover musically meaningful patterns

## License

This project is licensed under the [CC-BY-4.0 License](https://creativecommons.org/licenses/by/4.0/).

## Acknowledgements

We gratefully acknowledge Polish high-performance computing infrastructure PLGrid (HPC Center: ACK Cyfronet AGH) for providing computer facilities and support within computational grant no. PLG/2025/018397.

## Citation

If you use ConceptCaps in your research, please cite:

```bibtex
@article{sienkiewicz2026conceptcaps,
  title={ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models},
  author={Sienkiewicz, Bruno and Neumann, Åukasz and Modrzejewski, Mateusz},
  journal={arXiv preprint arXiv:2601.14157},
  year={2026}
}
```

## Authors

- [Bruno Sienkiewicz](https://github.com/bsienkiewicz)
- [Åukasz Neumann](https://arxiv.org/search/cs?searchtype=author&query=Neumann,+%C5%81)
- [Mateusz Modrzejewski](https://arxiv.org/search/cs?searchtype=author&query=Modrzejewski,+M)