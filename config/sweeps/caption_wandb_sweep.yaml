project: caption_fine_tuning
program: src/scripts/caption/run_fine_tuning_lightning.py
method: bayes
metric:
  name: test/mauve
  goal: maximize
parameters:
  paths:
    value: plgrid
  logger:
    value: wandb
  callbacks:
    value: [early_stopping, rich_progress_bar]
  callbacks.early_stopping.monitor:
    value: val/bertscore_f1
  callbacks.early_stopping.patience:
    value: 3
  callbacks.early_stopping.mode:
    value: max
  logger.wandb.log_model:
    value: false
  model:
    value: caption/llama_3.1_8b_instruct
  prompt:
    values: [llama, llama_refined]
  data.batch_size:
    value: 4
  trainer.accumulate_grad_batches:
    values: [2, 4, 8, 16]
  trainer.optimizer.lr:
    distribution: log_uniform_values
    min: 5e-6
    max: 1e-4
  trainer.optimizer.weight_decay:
    distribution: uniform
    min: 0.0
    max: 0.1
  lora.r:
    values: [8, 16, 32, 64]
  lora.lora_alpha:
    values: [8, 16, 32]
  lora.lora_dropout:
    distribution: uniform
    min: 0.1
    max: 0.3
  generation.max_new_tokens:
    value: 128

early_terminate:
  type: hyperband
  min_iter: 3

command:
  - ${env}
  - python
  - ${program}
  - ${args_no_hyphens}
