# @package _global_
# Preset for running full pipeline with large models on PLGrid

defaults:
  - override /paths: plgrid
  - override /logger: wandb

# Fine-tuning stage overrides
fine_tuning:
  model:
    name: meta-llama/Llama-3.1-8B-Instruct
    device_map: auto

  data:
    batch_size: 2
    dataloader_num_workers: 4
    max_train_samples: null
    max_val_samples: null
    max_test_samples: null

  trainer:
    max_epochs: 5
    accelerator: gpu
    devices: auto
    strategy: ddp
    precision: bf16
    accumulate_grad_batches: 4
    enable_progress_bar: false

  lora:
    r: 32
    lora_alpha: 64

# Inference stage overrides
inference:
  model:
    name: meta-llama/Llama-3.1-8B-Instruct

  data:
    batch_size: 4
    max_test_samples: null

# TTA stage overrides
tta:
  model:
    name: facebook/musicgen-large
    compile: true
    device_map: null

  data:
    batch_size: 16
    max_sequence_length: 1503

  generation:
    use_accelerator: true
    guidance_scale: 3.0

  evaluation:
    skip_evaluation: true

  target_split: test
  max_samples: null

# Callbacks
callbacks:
  model_checkpoint:
    monitor: val/bertscore_f1
    mode: max
    save_top_k: 1
    filename: best-checkpoint

  early_stopping:
    monitor: val/bertscore_f1
    mode: max
    patience: 5

# Logger
logger:
  wandb:
    project: full_pipeline
    log_model: false
