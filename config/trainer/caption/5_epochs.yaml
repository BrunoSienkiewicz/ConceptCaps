per_device_train_batch_size: 32
gradient_accumulation_steps: 2
learning_rate: 0.001
num_train_epochs: 5
optim: adamw_8bit
weight_decay: 0.05
lr_scheduler_type: cosine
seed: ${random_state}
output_dir: ${paths.output_dir}/trainer
report_to:
  - wandb
logging_steps: 1
logging_strategy: steps
save_strategy: best
load_best_model_at_end: true
save_only_model: false
fp16: null
bf16: null
remove_unused_columns: false
eval_strategy: epoch
save_total_limit: 1
