_target_: lightning.pytorch.Trainer
max_epochs: 15
per_device_train_batch_size: 32
gradient_accumulation_steps: 2
dataloader_num_workers: 4
accelerator: auto
devices: auto
strategy: ddp
precision: bf16
gradient_clip_val: 1.0
log_every_n_steps: 5
val_check_interval: 1.0
check_val_every_n_epoch: 1
enable_progress_bar: true
enable_model_summary: true
deterministic: true
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0005
  weight_decay: 0.05
  betas:
    - 0.9
    - 0.999
  eps: 1e-08
lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 5
  eta_min: 0