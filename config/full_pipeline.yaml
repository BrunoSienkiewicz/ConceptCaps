# Full Pipeline Configuration
# Combines: Fine-tuning → Inference → TTA Generation

defaults:
  - paths: default
  - logger: null
  - callbacks: default
  - hydra: default

# Global settings
run_id: ${now:%Y-%m-%d_%H-%M-%S}
task_name: full_pipeline
device: cuda
random_state: 42

# Stages to run (comment out to skip)
stages:
  - fine_tuning
  - inference
  - tta_generation

# Optional: path to pre-generated captions (skip inference stage)
# captions_path: null

# =============================================================================
# STAGE 1: Fine-tuning Configuration
# =============================================================================
fine_tuning:
  model:
    name: meta-llama/Llama-3.1-8B-Instruct
    checkpoint_dir: ""
    device_map: auto
    trust_remote_code: true
    quantization:
      load_in_4bit: true
      bnb_4bit_use_double_quant: true
      bnb_4bit_quant_type: nf4
      bnb_4bit_compute_dtype: bfloat16
    tokenizer:
      padding_side: right
      pad_token_as_eos: true

  data:
    dataset_name: google/MusicCaps
    aspect_column: aspect_list
    caption_column: caption
    text_column: text
    id_column: ytid
    batch_size: 4
    dataloader_num_workers: 4
    max_train_samples: null
    max_val_samples: null
    max_test_samples: null

  prompt:
    template: "{system_prompt}\n\n{user_prompt}"
    eval_template: "{system_prompt}\n\n{user_prompt}"
    prompt_delimiter: "\n\n"
    system_prompt: "You are a music description expert. Generate detailed, accurate descriptions of music based on the given aspects."
    user_prompt_template: "Generate a detailed music description based on these aspects: {aspects}"

  lora:
    r: 16
    lora_alpha: 32
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LM

  trainer:
    max_epochs: 3
    accelerator: auto
    devices: auto
    strategy: ddp
    precision: bf16
    gradient_clip_val: 1.0
    accumulate_grad_batches: 1
    log_every_n_steps: 10
    val_check_interval: 1.0
    check_val_every_n_epoch: 1
    enable_progress_bar: true
    enable_model_summary: false
    deterministic: false
    optimizer:
      _target_: torch.optim.AdamW
      lr: 2e-4
      weight_decay: 0.01
    lr_scheduler:
      _target_: torch.optim.lr_scheduler.CosineAnnealingLR
      T_max: ${fine_tuning.trainer.max_epochs}
      eta_min: 1e-6

  evaluation:
    metrics:
      - name: bertscore
        kwargs:
          lang: en
      - name: bleu
        kwargs: {}
    output_predictions: true
    compute_perplexity: false
    compute_llm_judge: false

  generation:
    max_new_tokens: 256
    max_length: 512
    temperature: 0.7
    top_k: 50
    top_p: 0.95
    do_sample: true
    repetition_penalty: 1.1

  run_test: true

# =============================================================================
# STAGE 2: Inference Configuration
# =============================================================================
inference:
  model:
    name: meta-llama/Llama-3.1-8B-Instruct
    checkpoint_dir: ""  # Will be set to fine-tuning output
    device_map: auto
    trust_remote_code: true
    quantization:
      load_in_4bit: true
      bnb_4bit_use_double_quant: true
      bnb_4bit_quant_type: nf4
      bnb_4bit_compute_dtype: bfloat16
    tokenizer:
      padding_side: right
      pad_token_as_eos: true

  data:
    dataset_name: brunoluz/vae-tags-dataset
    aspect_column: aspect_list
    caption_column: caption
    text_column: text
    id_column: id
    batch_size: 8
    dataloader_num_workers: 4
    max_train_samples: null
    max_val_samples: null
    max_test_samples: null

  prompt:
    template: "{system_prompt}\n\n{user_prompt}"
    eval_template: "{system_prompt}\n\n{user_prompt}"
    prompt_delimiter: "\n\n"
    system_prompt: "You are a music description expert. Generate detailed, accurate descriptions of music based on the given aspects."
    user_prompt_template: "Generate a detailed music description based on these aspects: {aspects}"

  lora:
    r: 16
    lora_alpha: 32
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LM

  trainer:
    optimizer:
      _target_: torch.optim.AdamW
      lr: 2e-4
    lr_scheduler:
      _target_: torch.optim.lr_scheduler.CosineAnnealingLR
      T_max: 3

  evaluation:
    metrics: []
    output_predictions: true
    compute_perplexity: false
    compute_llm_judge: false

  generation:
    max_new_tokens: 256
    max_length: 512
    temperature: 0.7
    top_k: 50
    top_p: 0.95
    do_sample: true
    repetition_penalty: 1.1

# =============================================================================
# STAGE 3: TTA (Text-to-Audio) Generation Configuration
# =============================================================================
tta:
  model:
    name: facebook/musicgen-small
    checkpoint_dir: ""
    device_map: auto
    trust_remote_code: true
    use_bf16: true
    gradient_checkpointing: true
    compile: false
    tokenizer:
      padding_side: right
      pad_token_as_eos: true
      max_new_tokens: 256

  data:
    caption_column: caption
    id_column: id
    batch_size: 4
    max_sequence_length: 256
    filename_template: "{}.wav"

  generation:
    temperature: 1.0
    top_k: 50
    top_p: 0.95
    do_sample: true
    guidance_scale: 3.0
    use_accelerator: false
    sample_rate: 32000

  evaluation:
    skip_evaluation: true
    clap_model: laion/clap-htsat-unfused
    fad_model: google/vggish

  # Which split to use for audio generation
  target_split: test
  # Maximum number of samples to generate (null for all)
  max_samples: null

# Logger configuration
logger:
  wandb:
    project: full_pipeline
    log_model: false
    tags:
      - pipeline
      - fine_tuning
      - inference
      - tta
