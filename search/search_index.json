{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ConceptCaps Documentation","text":"<p>Welcome to the ConceptCaps documentation. ConceptCaps is a distilled concept dataset for interpretability research in music models.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Paper: arXiv:2601.14157</li> <li>Dataset: Hugging Face</li> <li>Repository: GitHub</li> </ul>"},{"location":"#documentation-contents","title":"Documentation Contents","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>README - Project overview and installation</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>ConceptCaps is a concept-based music captioning dataset designed for interpretability research in text-to-audio (TTA) generation systems. The dataset provides:</p> <ul> <li>21,000+ music-caption-tags triplets with explicit labels</li> <li>200-attribute taxonomy covering genre, mood, instruments, and tempo</li> <li>Structured annotations for concept-based analysis</li> </ul>"},{"location":"#pipeline-summary","title":"Pipeline Summary","text":"<ol> <li>Taxonomy Extraction: Map MusicCaps tags to structured categories</li> <li>VAE Sampling: Learn and sample plausible attribute combinations</li> <li>LLM Captioning: Generate natural language descriptions from attributes</li> <li>Audio Synthesis: Create audio using MusicGen</li> </ol>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#concept-separation","title":"Concept Separation","text":"<p>Unlike existing datasets, ConceptCaps provides clean separation between:</p> <ul> <li>Genre concepts (jazz, rock, classical, etc.)</li> <li>Mood concepts (happy, sad, energetic, etc.)</li> <li>Instrument concepts (piano, guitar, drums, etc.)</li> <li>Tempo concepts (slow, fast, moderate, etc.)</li> </ul>"},{"location":"#tcav-compatibility","title":"TCAV Compatibility","text":"<p>The dataset is designed for Testing with Concept Activation Vectors (TCAV), enabling:</p> <ul> <li>Concept probe training</li> <li>Model interpretability analysis</li> <li>Controlled generation experiments</li> </ul>"},{"location":"#quality-validation","title":"Quality Validation","text":"<p>All data is validated through:</p> <ul> <li>CLAP: Audio-text alignment scores</li> <li>BERTScore: Caption semantic quality</li> <li>MAUVE: Distribution comparison</li> <li>TCAV: Concept separability verification</li> </ul>"},{"location":"#citation","title":"Citation","text":"<pre><code>@article{sienkiewicz2026conceptcaps,\n  title={ConceptCaps: a Distilled Concept Dataset for Interpretability in Music Models},\n  author={Sienkiewicz, Bruno and Neumann, \u0141ukasz and Modrzejewski, Mateusz},\n  journal={arXiv preprint arXiv:2601.14157},\n  year={2026}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under CC-BY-4.0.</p>"},{"location":"config/","title":"Configuration","text":"<p>Hydra configuration files for all pipeline stages.</p>"},{"location":"config/#overview","title":"Overview","text":"<p>The project uses Hydra for hierarchical configuration management. Each training/inference script has a main config that composes modular sub-configs.</p>"},{"location":"config/#main-configuration-files","title":"Main Configuration Files","text":"File Purpose <code>vae_training.yaml</code> VAE training for attribute modeling <code>caption_fine_tuning.yaml</code> LLM LoRA fine-tuning <code>caption_inference.yaml</code> Caption generation <code>tta_inference.yaml</code> Audio synthesis with MusicGen"},{"location":"config/#configuration-structure","title":"Configuration Structure","text":"<pre><code>config/\n\u251c\u2500\u2500 callbacks/          # Training callbacks (checkpoints, early stopping)\n\u251c\u2500\u2500 data/               # Dataset configurations\n\u2502   \u251c\u2500\u2500 caption/        # Caption dataset configs\n\u2502   \u251c\u2500\u2500 tta/            # Audio generation configs\n\u2502   \u2514\u2500\u2500 vae/            # VAE training data configs\n\u251c\u2500\u2500 evaluation/         # Metric configurations\n\u251c\u2500\u2500 generation/         # Generation parameters\n\u251c\u2500\u2500 logger/             # Logging (W&amp;B) configs\n\u251c\u2500\u2500 lora/               # LoRA adapter configs\n\u251c\u2500\u2500 model/              # Model architecture configs\n\u2502   \u251c\u2500\u2500 caption/        # LLM model configs\n\u2502   \u251c\u2500\u2500 tta/            # MusicGen configs\n\u2502   \u2514\u2500\u2500 vae/            # VAE architecture configs\n\u251c\u2500\u2500 paths/              # Path configurations\n\u251c\u2500\u2500 preset/             # Experiment presets\n\u251c\u2500\u2500 prompt/             # LLM prompt templates\n\u251c\u2500\u2500 sweeps/             # Hyperparameter sweep configs\n\u2514\u2500\u2500 trainer/            # PyTorch Lightning trainer configs\n</code></pre>"},{"location":"config/#usage","title":"Usage","text":""},{"location":"config/#override-parameters-from-cli","title":"Override parameters from CLI","text":"<pre><code>python -m src.scripts.vae_training trainer.max_epochs=100 data.batch_size=64\n</code></pre>"},{"location":"config/#use-a-preset","title":"Use a preset","text":"<pre><code>python -m src.scripts.caption_fine_tuning +preset=caption/quick_test\n</code></pre>"},{"location":"config/#key-configuration-options","title":"Key Configuration Options","text":""},{"location":"config/#vae-training","title":"VAE Training","text":"<pre><code># vae_training.yaml\nmodel:\n  latent_dim: 32       # Latent space dimension\n  hidden_dim: 128      # Hidden layer size\n  beta: 1.0            # KL divergence weight\n\nloss:\n  bce_weight: 1.0      # Reconstruction loss weight\n  kld_weight: 1.0      # KL divergence weight\n\ninference:\n  num_samples: 5000    # Samples to generate\n  temperature: 1.0     # Sampling temperature\n  threshold: 0.5       # Tag activation threshold\n</code></pre>"},{"location":"config/#caption-fine-tuning","title":"Caption Fine-tuning","text":"<pre><code># caption_fine_tuning.yaml\nmodel:\n  name: meta-llama/Llama-3.1-8B-Instruct\n  quantization:\n    load_in_4bit: true\n    bnb_4bit_quant_type: nf4\n\nlora:\n  r: 16                # LoRA rank\n  lora_alpha: 32       # LoRA alpha\n  target_modules:      # Modules to adapt\n    - q_proj\n    - v_proj\n</code></pre>"},{"location":"config/#tta-inference","title":"TTA Inference","text":"<pre><code># tta_inference.yaml\nmodel:\n  name: facebook/musicgen-small\n  \ngeneration:\n  max_new_tokens: 512  # ~10 seconds of audio\n  guidance_scale: 3.0  # Classifier-free guidance\n  temperature: 1.0\n</code></pre>"},{"location":"data/","title":"Data","text":"<p>Datasets and intermediate data files.</p>"},{"location":"data/#directory-structure","title":"Directory Structure","text":"<pre><code>data/\n\u251c\u2500\u2500 concepts_to_tags.json              # Taxonomy mapping\n\u251c\u2500\u2500 musiccaps_tag_frequencies.csv      # Tag frequencies in MusicCaps\n\u251c\u2500\u2500 tcav_genre_classification_results.json  # TCAV analysis results\n\u2502\n\u251c\u2500\u2500 evaluation_results/                # Metric outputs\n\u2502   \u251c\u2500\u2500 base.csv                       # Base LLM results\n\u2502   \u251c\u2500\u2500 ft.csv                         # Fine-tuned LLM results\n\u2502   \u2514\u2500\u2500 zero_shot.csv                  # Zero-shot results\n\u2502\n\u251c\u2500\u2500 generated_captions/                # LLM outputs\n\u2502   \u251c\u2500\u2500 final_train.csv\n\u2502   \u251c\u2500\u2500 final_validation.csv\n\u2502   \u2514\u2500\u2500 final_test.csv\n\u2502\n\u251c\u2500\u2500 mtg_jamendo/                       # MTG-Jamendo dataset\n\u2502   \u2514\u2500\u2500 autotagging_top50tags_processed_cleaned.csv\n\u2502\n\u251c\u2500\u2500 musiccaps-tags-to-caption/         # DIstilled MusicCaps dataset\n\u2502   \u2514\u2500\u2500 all.csv\n\u2502\n\u251c\u2500\u2500 random-tags-dataset/               # Random tag combinations\n\u2514\u2500\u2500 vae-tags-dataset/                  # VAE-sampled combinations\n</code></pre>"},{"location":"notebooks/","title":"Notebooks","text":"<p>Jupyter notebooks demonstrating each pipeline stage.</p>"},{"location":"notebooks/#pipeline-overview","title":"Pipeline Overview","text":"<p>The notebooks follow the dataset creation pipeline sequentially:</p> <pre><code>1. Taxonomy \u2192 2. VAE \u2192 3. Comparison \u2192 4. Captions \u2192 5. Audio \u2192 6. Final \u2192 7. TCAV\n</code></pre>"},{"location":"notebooks/#notebook-descriptions","title":"Notebook Descriptions","text":""},{"location":"notebooks/#1-taxonomy-and-dataset-distillation","title":"1. Taxonomy and Dataset Distillation","text":"<p>File: <code>1. Taxonomy and dataset distillation.ipynb</code></p> <p>Creates the 200-tag taxonomy from MusicCaps annotations:</p> <ul> <li>Extract and clean tags from MusicCaps</li> <li>Map tags to concept categories (genre, mood, instrument, tempo)</li> <li>Analyze tag co-occurrence patterns</li> <li>Export <code>concepts_to_tags.json</code></li> </ul>"},{"location":"notebooks/#2-vae-aspect-modeling","title":"2. VAE Aspect Modeling","text":"<p>File: <code>2. VAE aspect modeling.ipynb</code></p> <p>Trains the Beta-VAE on tag distributions:</p> <ul> <li>Prepare multi-hot encoded tag vectors</li> <li>Visualize latent space</li> <li>Sample new attribute combinations</li> </ul>"},{"location":"notebooks/#3-musiccaps-and-vae-generated-dataset-comparison","title":"3. MusicCaps and VAE Generated Dataset Comparison","text":"<p>File: <code>3. MusicCaps and VAE generated dataset comparison.ipynb</code></p> <p>Validates VAE sampling quality:</p> <ul> <li>Compare tag distributions (original vs. sampled)</li> <li>Analyze co-occurrence preservation</li> <li>Statistical tests for distribution similarity</li> </ul>"},{"location":"notebooks/#4-conditioned-caption-inference","title":"4. Conditioned Caption Inference","text":"<p>File: <code>4. Conditioned caption inference.ipynb</code></p> <p>Evaluates captions from the fine-tuned LLM against baselines:</p> <ul> <li>Analyze generated captions from VAE-sampled attributes</li> <li>Evaluate caption quality (perplexity, diversity)</li> </ul>"},{"location":"notebooks/#5-audio-generation-analysis","title":"5. Audio Generation Analysis","text":"<p>File: <code>5. Audio generation analysis.ipynb</code></p> <p>Analyzes MusicGen audio synthesis:</p> <ul> <li>Compute CLAP and FAD scores</li> <li>Quality assessment</li> </ul>"},{"location":"notebooks/#6-create-final-datasets","title":"6. Create Final Datasets","text":"<p>File: <code>6. Create final datasets.ipynb</code></p> <p>Assembles the final ConceptCaps dataset:</p> <ul> <li>Combine captions, audio paths, and metadata</li> <li>Create train/validation/test splits</li> <li>Export to HuggingFace format</li> </ul>"},{"location":"notebooks/#7-tcav-for-genre-classification","title":"7. TCAV for Genre Classification","text":"<p>File: <code>7. TCAV for genre classification.ipynb</code></p> <p>Interpretability analysis with TCAV:</p> <ul> <li>Train genre classifier on GTZAN</li> <li>Extract bottleneck activations</li> <li>Train CAVs for each concept</li> <li>Compute TCAV scores</li> <li>Visualize concept importance</li> </ul>"},{"location":"code/caption/","title":"Caption Module","text":"<p>LLM fine-tuning for music caption generation from concept tags.</p> <p>This module handles the conversion of structured music attributes (tags) into natural language descriptions using fine-tuned language models with LoRA adapters.</p>"},{"location":"code/caption/#overview","title":"Overview","text":"<p>The caption pipeline:</p> <ol> <li>Data preparation: Format prompts with attribute lists</li> <li>Fine-tuning: Train LLM with LoRA on MusicCaps-derived data  </li> <li>Inference: Generate captions from new attribute combinations</li> </ol>"},{"location":"code/caption/#configuration","title":"Configuration","text":"<p>Configuration dataclasses for audio caption generation.</p> <p>Classes:</p> Name Description <code>CaptionGenerationConfig</code> <code>ModelConfig</code> <code>DatasetConfig</code> <code>TrainerConfig</code> <code>LoRAConfig</code> <code>QuantizationConfig</code>"},{"location":"code/caption/#caption.config.CaptionGenerationConfig","title":"<code>CaptionGenerationConfig(model, data, prompt, lora, trainer, evaluation, paths, generation, device='cuda', random_state=42, run_id='default_run', task_name='caption_generation')</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DictConfig</code></p>"},{"location":"code/caption/#caption.config.ModelConfig","title":"<code>ModelConfig(name, checkpoint_dir='', device_map='auto', trust_remote_code=True, quantization=QuantizationConfig(), tokenizer=ModelTokenizerConfig())</code>  <code>dataclass</code>","text":""},{"location":"code/caption/#caption.config.DatasetConfig","title":"<code>DatasetConfig(dataset_name='', aspect_column='aspect_list', caption_column='caption', text_column='text', id_column='id', batch_size=DEFAULT_BATCH_SIZE, dataloader_num_workers=4, remove_columns=None, max_train_samples=None, max_val_samples=None, max_test_samples=None)</code>  <code>dataclass</code>","text":""},{"location":"code/caption/#caption.config.TrainerConfig","title":"<code>TrainerConfig(max_epochs=3, dataloader_num_workers=1, accelerator='auto', devices='auto', strategy='ddp', precision='bf16', gradient_clip_val=DEFAULT_GRADIENT_CLIP_VAL, accumulate_grad_batches=1, log_every_n_steps=5, val_check_interval=1.0, check_val_every_n_epoch=1, enable_progress_bar=False, enable_model_summary=False, deterministic=False, optimizer=(lambda: DictConfig({}))(), lr_scheduler=(lambda: DictConfig({}))())</code>  <code>dataclass</code>","text":""},{"location":"code/caption/#caption.config.LoRAConfig","title":"<code>LoRAConfig(r=16, lora_alpha=32, target_modules=list(), lora_dropout=0.05, bias='none', task_type='CAUSAL_LM')</code>  <code>dataclass</code>","text":""},{"location":"code/caption/#caption.config.QuantizationConfig","title":"<code>QuantizationConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype='bfloat16')</code>  <code>dataclass</code>","text":""},{"location":"code/caption/#captionfinetuningmodule","title":"CaptionFineTuningModule","text":""},{"location":"code/caption/#caption.lightning_module.CaptionFineTuningModule","title":"<code>caption.lightning_module.CaptionFineTuningModule(model_cfg, generation_cfg, lora_cfg, optimizer_cfg, lr_scheduler_cfg, prompt_cfg, tokenizer, metric_computer=None)</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for caption fine-tuning.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the model.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> <code>configure_optimizers</code> <p>Configure optimizer and learning rate scheduler.</p>"},{"location":"code/caption/#caption.lightning_module.CaptionFineTuningModule.forward","title":"<code>forward(input_ids, attention_mask, labels=None)</code>","text":"<p>Forward pass through the model.</p>"},{"location":"code/caption/#caption.lightning_module.CaptionFineTuningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p>"},{"location":"code/caption/#caption.lightning_module.CaptionFineTuningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p>"},{"location":"code/caption/#caption.lightning_module.CaptionFineTuningModule.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizer and learning rate scheduler.</p>"},{"location":"code/caption/#captiondatamodule","title":"CaptionDataModule","text":""},{"location":"code/caption/#caption.lightning_datamodule.CaptionDataModule","title":"<code>caption.lightning_datamodule.CaptionDataModule(dataset, tokenizer, data_cfg, prompt_cfg, batch_size=8, num_workers=4, max_length=512)</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>PyTorch Lightning DataModule for caption fine-tuning.</p>"},{"location":"code/caption/#data-processing","title":"Data Processing","text":"<p>Data processing utilities for caption fine-tuning.</p> <p>Provides functions to format prompts and prepare datasets for both training and inference using configurable prompt templates.</p> <p>Functions:</p> Name Description <code>prepare_datasets</code> <p>Prepare datasets for training by formatting prompts with reference captions.</p> <code>prepare_inference_datasets</code> <p>Prepare datasets for inference (without reference captions).</p>"},{"location":"code/caption/#caption.data.prepare_datasets","title":"<code>prepare_datasets(data_cfg, prompt_cfg, raw_dataset)</code>","text":"<p>Prepare datasets for training by formatting prompts with reference captions.</p> <p>Parameters:</p> Name Type Description Default <p>Data configuration with column names.</p> required <p>Prompt template configuration.</p> required <code>DatasetDict</code> <p>Raw HuggingFace DatasetDict.</p> required <p>Returns:</p> Type Description <code>DatasetDict</code> <p>Processed DatasetDict with formatted text column.</p>"},{"location":"code/caption/#caption.data.prepare_datasets(data_cfg)","title":"<code>data_cfg</code>","text":""},{"location":"code/caption/#caption.data.prepare_datasets(prompt_cfg)","title":"<code>prompt_cfg</code>","text":""},{"location":"code/caption/#caption.data.prepare_datasets(raw_dataset)","title":"<code>raw_dataset</code>","text":""},{"location":"code/caption/#caption.data.prepare_inference_datasets","title":"<code>prepare_inference_datasets(data_cfg, prompt_cfg, raw_dataset)</code>","text":"<p>Prepare datasets for inference (without reference captions).</p> <p>Parameters:</p> Name Type Description Default <p>Data configuration with column names.</p> required <p>Prompt template configuration.</p> required <code>DatasetDict</code> <p>Raw HuggingFace DatasetDict.</p> required <p>Returns:</p> Type Description <code>DatasetDict</code> <p>Processed DatasetDict with formatted prompts for generation.</p>"},{"location":"code/caption/#caption.data.prepare_inference_datasets(data_cfg)","title":"<code>data_cfg</code>","text":""},{"location":"code/caption/#caption.data.prepare_inference_datasets(prompt_cfg)","title":"<code>prompt_cfg</code>","text":""},{"location":"code/caption/#caption.data.prepare_inference_datasets(raw_dataset)","title":"<code>raw_dataset</code>","text":""},{"location":"code/caption/#model-utilities","title":"Model Utilities","text":"<p>Caption model utilities for LLM fine-tuning.</p> <p>Provides functions to prepare language models with LoRA adapters and quantization for efficient caption generation fine-tuning.</p> <p>Functions:</p> Name Description <code>prepare_tokenizer</code> <p>Prepare tokenizer with custom padding configuration.</p> <code>prepare_training_model</code> <p>Prepare model for LoRA fine-tuning with optional quantization.</p> <code>build_quantization_config</code> <p>Build BitsAndBytes quantization config from Hydra config.</p>"},{"location":"code/caption/#caption.model.prepare_tokenizer","title":"<code>prepare_tokenizer(model_cfg)</code>","text":"<p>Prepare tokenizer with custom padding configuration.</p> <p>Parameters:</p> Name Type Description Default <code>DictConfig</code> <p>Model configuration with tokenizer settings.</p> required <p>Returns:</p> Type Description <code>AutoTokenizer</code> <p>Configured tokenizer instance.</p>"},{"location":"code/caption/#caption.model.prepare_tokenizer(model_cfg)","title":"<code>model_cfg</code>","text":""},{"location":"code/caption/#caption.model.prepare_training_model","title":"<code>prepare_training_model(model_cfg, lora_cfg)</code>","text":"<p>Prepare model for LoRA fine-tuning with optional quantization.</p> <p>Loads a pretrained model, applies quantization if configured, and adds LoRA adapters. Supports loading from existing checkpoints.</p> <p>Parameters:</p> Name Type Description Default <code>DictConfig</code> <p>Model configuration (name, device_map, checkpoint_dir).</p> required <code>DictConfig</code> <p>LoRA adapter configuration.</p> required <p>Returns:</p> Type Description <code>AutoModelForCausalLM</code> <p>Model with LoRA adapters ready for fine-tuning.</p>"},{"location":"code/caption/#caption.model.prepare_training_model(model_cfg)","title":"<code>model_cfg</code>","text":""},{"location":"code/caption/#caption.model.prepare_training_model(lora_cfg)","title":"<code>lora_cfg</code>","text":""},{"location":"code/caption/#caption.model.build_quantization_config","title":"<code>build_quantization_config(model_cfg)</code>","text":"<p>Build BitsAndBytes quantization config from Hydra config.</p> <p>Parameters:</p> Name Type Description Default <code>DictConfig</code> <p>Model configuration with optional quantization settings.</p> required <p>Returns:</p> Type Description <code>BitsAndBytesConfig | None</code> <p>Quantization config or None if not specified.</p>"},{"location":"code/caption/#caption.model.build_quantization_config(model_cfg)","title":"<code>model_cfg</code>","text":""},{"location":"code/caption/#evaluation","title":"Evaluation","text":"<p>Classes:</p> Name Description <code>MetricComputer</code> <p>Computes evaluation metrics for generated captions.</p> <p>Functions:</p> Name Description <code>calculate_perplexity</code> <p>Calculate perplexity for a given text.</p> <code>evaluate_with_llm_judge</code> <p>Evaluate generated captions using an LLM as a judge.</p>"},{"location":"code/caption/#caption.evaluation.MetricComputer","title":"<code>MetricComputer(metric_cfgs, tokenizer=None)</code>","text":"<p>Computes evaluation metrics for generated captions.</p> <p>Methods:</p> Name Description <code>compute_metrics</code> <p>Compute metrics given model outputs and references.</p> <code>save_predictions</code> <p>Save predictions and metrics to output directory.</p>"},{"location":"code/caption/#caption.evaluation.MetricComputer._compute_metrics","title":"<code>_compute_metrics(predictions, references)</code>","text":"<p>Calculate metrics comparing predictions to references.</p>"},{"location":"code/caption/#caption.evaluation.MetricComputer.compute_metrics","title":"<code>compute_metrics(predictions, references)</code>","text":"<p>Compute metrics given model outputs and references.</p>"},{"location":"code/caption/#caption.evaluation.MetricComputer.save_predictions","title":"<code>save_predictions(output_dir)</code>","text":"<p>Save predictions and metrics to output directory.</p> <p>Files saved: - /evaluation_metrics.json - /all_predictions.csv <p>Parameters:</p> Name Type Description Default <code>Path</code> <p>Directory to save predictions file</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with evaluation metrics</p>"},{"location":"code/caption/#caption.evaluation.MetricComputer.save_predictions(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"code/caption/#caption.evaluation.calculate_perplexity","title":"<code>calculate_perplexity(model, tokenizer, text, device=None)</code>","text":"<p>Calculate perplexity for a given text.</p> <p>Parameters:</p> Name Type Description Default <code>AutoModelForCausalLM</code> <p>Language model</p> required <code>AutoTokenizer</code> <p>Tokenizer</p> required <code>str</code> <p>Text to calculate perplexity for</p> required <code>str</code> <p>Device to use (defaults to model's device)</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Perplexity score (lower is better)</p>"},{"location":"code/caption/#caption.evaluation.calculate_perplexity(model)","title":"<code>model</code>","text":""},{"location":"code/caption/#caption.evaluation.calculate_perplexity(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"code/caption/#caption.evaluation.calculate_perplexity(text)","title":"<code>text</code>","text":""},{"location":"code/caption/#caption.evaluation.calculate_perplexity(device)","title":"<code>device</code>","text":""},{"location":"code/caption/#caption.evaluation.evaluate_with_llm_judge","title":"<code>evaluate_with_llm_judge(captions, prompts, judge_model_name='meta-llama/Llama-3.2-3B-Instruct', judge_template=None, batch_size=4, device='cuda')</code>","text":"<p>Evaluate generated captions using an LLM as a judge. Based on: https://huggingface.co/learn/cookbook/llm_judge</p> <p>Parameters:</p> Name Type Description Default <code>list[str]</code> <p>Generated captions to evaluate</p> required <code>list[str]</code> <p>Original prompts used for generation</p> required <code>str</code> <p>Name of the judge model to use</p> <code>'meta-llama/Llama-3.2-3B-Instruct'</code> <code>int</code> <p>Batch size for judge evaluation</p> <code>4</code> <code>str</code> <p>Device to use</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with evaluation scores and reasoning</p>"},{"location":"code/caption/#caption.evaluation.evaluate_with_llm_judge(captions)","title":"<code>captions</code>","text":""},{"location":"code/caption/#caption.evaluation.evaluate_with_llm_judge(prompts)","title":"<code>prompts</code>","text":""},{"location":"code/caption/#caption.evaluation.evaluate_with_llm_judge(judge_model_name)","title":"<code>judge_model_name</code>","text":""},{"location":"code/caption/#caption.evaluation.evaluate_with_llm_judge(batch_size)","title":"<code>batch_size</code>","text":""},{"location":"code/caption/#caption.evaluation.evaluate_with_llm_judge(device)","title":"<code>device</code>","text":""},{"location":"code/caption/#inference","title":"Inference","text":"<p>Functions:</p> Name Description <code>generate_captions_batch</code> <p>Generate captions for a batch of prompts efficiently with optional</p>"},{"location":"code/caption/#caption.inference.generate_captions_batch","title":"<code>generate_captions_batch(model, tokenizer, prompts, batch_size=8, compute_perplexity=False, compute_llm_judge=False, llm_judge_config=None)</code>","text":"<p>Generate captions for a batch of prompts efficiently with optional quality metrics.</p> <p>Parameters:</p> Name Type Description Default <code>AutoModelForCausalLM</code> <p>Language model for generation</p> required <code>AutoTokenizer</code> <p>Tokenizer for encoding/decoding</p> required <code>list[str]</code> <p>List of input prompts</p> required <p>Generation configuration</p> required <code>int</code> <p>Number of prompts to process per batch</p> <code>8</code> <code>bool</code> <p>Whether to compute perplexity for each generated caption</p> <code>False</code> <code>bool</code> <p>Whether to evaluate with LLM-as-a-judge</p> <code>False</code> <code>dict[str, Any] | None</code> <p>Configuration for LLM judge (model_name, device, etc.)</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[list[str], dict[str, Any] | None]</code> <p>Tuple of (generated captions, quality metrics dict or None)</p>"},{"location":"code/caption/#caption.inference.generate_captions_batch(model)","title":"<code>model</code>","text":""},{"location":"code/caption/#caption.inference.generate_captions_batch(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"code/caption/#caption.inference.generate_captions_batch(prompts)","title":"<code>prompts</code>","text":""},{"location":"code/caption/#caption.inference.generate_captions_batch(generate_cfg)","title":"<code>generate_cfg</code>","text":""},{"location":"code/caption/#caption.inference.generate_captions_batch(batch_size)","title":"<code>batch_size</code>","text":""},{"location":"code/caption/#caption.inference.generate_captions_batch(compute_perplexity)","title":"<code>compute_perplexity</code>","text":""},{"location":"code/caption/#caption.inference.generate_captions_batch(compute_llm_judge)","title":"<code>compute_llm_judge</code>","text":""},{"location":"code/caption/#caption.inference.generate_captions_batch(llm_judge_config)","title":"<code>llm_judge_config</code>","text":""},{"location":"code/reference/","title":"Code Reference","text":"<p>API documentation for the ConceptCaps source code.</p>"},{"location":"code/reference/#modules","title":"Modules","text":"Module Description Caption LLM fine-tuning for caption generation VAE Attribute co-occurrence modeling TCAV Concept-based interpretability TTA Text-to-audio generation Utilities Logging and experiment tracking"},{"location":"code/reference/#constants_1","title":"Constants","text":"<p>Project-wide configuration values.</p> <p>Project-wide constants and configuration paths.</p> <p>Contains default values for model architectures, training hyperparameters, and paths for data, models, and outputs. Most values can be overridden via environment variables.</p> <p>Attributes:</p> Name Type Description <code>PROJECT_ROOT</code> <code>DATA_DIR</code> <code>MODELS_DIR</code> <code>OUTPUTS_DIR</code> <code>GTZAN_GENRES</code> <code>Tuple[str, ...]</code>"},{"location":"code/reference/#constants.PROJECT_ROOT","title":"<code>PROJECT_ROOT = Path(os.getenv('PROJECT_ROOT', Path(__file__).parent.parent.absolute()))</code>  <code>module-attribute</code>","text":""},{"location":"code/reference/#constants.DATA_DIR","title":"<code>DATA_DIR = Path(os.getenv('DATA_DIR', PROJECT_ROOT / 'data'))</code>  <code>module-attribute</code>","text":""},{"location":"code/reference/#constants.MODELS_DIR","title":"<code>MODELS_DIR = Path(os.getenv('MODELS_DIR', PROJECT_ROOT / 'models'))</code>  <code>module-attribute</code>","text":""},{"location":"code/reference/#constants.OUTPUTS_DIR","title":"<code>OUTPUTS_DIR = Path(os.getenv('OUTPUTS_DIR', PROJECT_ROOT / 'outputs'))</code>  <code>module-attribute</code>","text":""},{"location":"code/reference/#constants.GTZAN_GENRES","title":"<code>GTZAN_GENRES = ('blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock')</code>  <code>module-attribute</code>","text":""},{"location":"code/tcav/","title":"TCAV Module","text":"<p>Testing with Concept Activation Vectors for model interpretability.</p> <p>This module implements TCAV analysis to quantify how much a classifier relies on human-defined concepts (genres, moods, instruments) when making predictions.</p>"},{"location":"code/tcav/#overview","title":"Overview","text":"<p>TCAV workflow:</p> <ol> <li>Extract activations from the classifier's bottleneck layer</li> <li>Train CAVs (linear classifiers) to separate concept vs. random examples</li> <li>Compute directional derivatives to measure concept influence on predictions</li> </ol>"},{"location":"code/tcav/#tcav","title":"TCAV","text":"<p>Main class for concept-based interpretability analysis.</p>"},{"location":"code/tcav/#tcav.tcav.TCAV","title":"<code>tcav.tcav.TCAV(model, device)</code>","text":"<p>Testing with Concept Activation Vectors for model interpretability.</p> <p>TCAV quantifies the importance of user-defined concepts for a classifier's predictions by learning concept activation vectors (CAVs) and measuring directional derivatives of predictions along these vectors.</p> <p>Parameters:</p> Name Type Description Default <code>MusicGenreClassifier</code> <p>Trained genre classifier with accessible bottleneck layer.</p> required <code>device</code> <p>Device to run computations on.</p> required Example <p>tcav = TCAV(model, device=\"cuda\") concept_acts = tcav.get_activations(concept_audio) random_acts = tcav.get_activations(random_audio) cav_result = tcav.train_cav(concept_acts, random_acts) tcav_score = tcav.compute_tcav_score(cav_result[\"cav\"], target_audio, target_class=0)</p> <p>Methods:</p> Name Description <code>get_activations</code> <p>Extract bottleneck layer activations from audio samples.</p> <code>train_cav</code> <p>Train Concept Activation Vector using linear SVM.</p> <code>get_directional_derivatives</code> <p>Compute directional derivatives of predictions along CAV direction.</p> <code>compute_tcav_score</code> <p>Compute TCAV score as fraction of samples positively aligned with</p>"},{"location":"code/tcav/#tcav.tcav.TCAV(model)","title":"<code>model</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV(device)","title":"<code>device</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.get_activations","title":"<code>get_activations(audio, batch_size=32)</code>","text":"<p>Extract bottleneck layer activations from audio samples.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Audio tensor of shape (n_samples, n_features).</p> required <code>int</code> <p>Batch size for processing.</p> <code>32</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Activations array of shape (n_samples, bottleneck_dim).</p>"},{"location":"code/tcav/#tcav.tcav.TCAV.get_activations(audio)","title":"<code>audio</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.get_activations(batch_size)","title":"<code>batch_size</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.train_cav","title":"<code>train_cav(concept_acts, random_acts, num_runs=DEFAULT_NUM_CAV_RUNS, test_size=0.2)</code>","text":"<p>Train Concept Activation Vector using linear SVM.</p> <p>Parameters:</p> Name Type Description Default <code>ndarray</code> <p>Activations from concept examples.</p> required <code>ndarray</code> <p>Activations from random/counter examples.</p> required <code>int</code> <p>Number of independent training runs for stability.</p> <code>DEFAULT_NUM_CAV_RUNS</code> <code>float</code> <p>Fraction of data for testing.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing 'cav' (mean vector), 'accuracy' (mean test accuracy),</p> <code>Dict[str, ndarray]</code> <p>and 'std' (standard deviation of accuracy across runs).</p>"},{"location":"code/tcav/#tcav.tcav.TCAV.train_cav(concept_acts)","title":"<code>concept_acts</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.train_cav(random_acts)","title":"<code>random_acts</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.train_cav(num_runs)","title":"<code>num_runs</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.train_cav(test_size)","title":"<code>test_size</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.get_directional_derivatives","title":"<code>get_directional_derivatives(audio, cav, target_class, batch_size=32)</code>","text":"<p>Compute directional derivatives of predictions along CAV direction.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Audio tensor to compute derivatives for.</p> required <code>ndarray</code> <p>Concept activation vector.</p> required <code>int</code> <p>Target class index for gradient computation.</p> required <code>int</code> <p>Batch size for processing.</p> <code>32</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of directional derivatives.</p>"},{"location":"code/tcav/#tcav.tcav.TCAV.get_directional_derivatives(audio)","title":"<code>audio</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.get_directional_derivatives(cav)","title":"<code>cav</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.get_directional_derivatives(target_class)","title":"<code>target_class</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.get_directional_derivatives(batch_size)","title":"<code>batch_size</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.compute_tcav_score","title":"<code>compute_tcav_score(activations, cav)</code>","text":"<p>Compute TCAV score as fraction of samples positively aligned with CAV.</p> <p>Parameters:</p> Name Type Description Default <code>ndarray</code> <p>Bottleneck activations of shape (n_samples, bottleneck_dim).</p> required <code>ndarray</code> <p>Concept activation vector.</p> required <p>Returns:</p> Type Description <code>float</code> <p>TCAV score between 0 and 1.</p>"},{"location":"code/tcav/#tcav.tcav.TCAV.compute_tcav_score(activations)","title":"<code>activations</code>","text":""},{"location":"code/tcav/#tcav.tcav.TCAV.compute_tcav_score(cav)","title":"<code>cav</code>","text":""},{"location":"code/tcav/#musicgenreclassifier","title":"MusicGenreClassifier","text":"<p>CNN-based classifier with exposed bottleneck layer for TCAV analysis.</p>"},{"location":"code/tcav/#tcav.model.MusicGenreClassifier","title":"<code>tcav.model.MusicGenreClassifier(num_genres=10)</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>CNN-based music genre classifier with accessible bottleneck layer.</p> <p>Architecture uses mel-spectrogram features with convolutional layers followed by a bottleneck layer (256 units) for TCAV analysis.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>Number of genre classes (default: 10 for GTZAN).</p> <code>10</code> <p>Methods:</p> Name Description <code>forward</code> <code>training_step</code> <code>validation_step</code> <code>configure_optimizers</code>"},{"location":"code/tcav/#tcav.model.MusicGenreClassifier(num_genres)","title":"<code>num_genres</code>","text":""},{"location":"code/tcav/#tcav.model.MusicGenreClassifier.forward","title":"<code>forward(x, return_bottleneck=True)</code>","text":""},{"location":"code/tcav/#tcav.model.MusicGenreClassifier.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":""},{"location":"code/tcav/#tcav.model.MusicGenreClassifier.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":""},{"location":"code/tcav/#tcav.model.MusicGenreClassifier.configure_optimizers","title":"<code>configure_optimizers()</code>","text":""},{"location":"code/tta/","title":"TTA Module","text":"<p>Text-to-audio generation using MusicGen.</p> <p>This module handles audio synthesis from generated captions using Facebook's MusicGen model, with support for multi-GPU inference.</p>"},{"location":"code/tta/#overview","title":"Overview","text":"<p>After generating captions from the LLM, this module:</p> <ol> <li>Tokenizes captions for MusicGen's text encoder</li> <li>Generates audio in batches (with GPU memory optimization)</li> <li>Saves audio files and computes quality metrics (CLAP, FAD)</li> </ol>"},{"location":"code/tta/#configuration","title":"Configuration","text":"<p>Configuration classes for Text-to-Audio generation pipeline.</p> <p>Classes:</p> Name Description <code>TTAConfig</code>"},{"location":"code/tta/#tta.config.TTAConfig","title":"<code>TTAConfig(model, data, evaluation, generation, logger, paths, random_state, batch_size=DEFAULT_BATCH_SIZE, device='cuda', run_id='default')</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DictConfig</code></p>"},{"location":"code/tta/#audio-generation","title":"Audio Generation","text":""},{"location":"code/tta/#single-gpu-generation","title":"Single-GPU Generation","text":""},{"location":"code/tta/#tta.audio.generate_audio_samples","title":"<code>tta.audio.generate_audio_samples(model, dataloader, audio_dir, max_new_tokens, batch_size, df, id_column='id', filename_template='{}.wav', temperature=1.0, top_k=DEFAULT_TOP_K, top_p=DEFAULT_TOP_P, do_sample=True, guidance_scale=DEFAULT_GUIDANCE_SCALE, sample_rate=DEFAULT_SAMPLE_RATE, loggers=None)</code>","text":""},{"location":"code/tta/#multi-gpu-generation-accelerate","title":"Multi-GPU Generation (Accelerate)","text":""},{"location":"code/tta/#tta.audio.generate_audio_samples_accelerate","title":"<code>tta.audio.generate_audio_samples_accelerate(model, dataloader, audio_dir, max_new_tokens, batch_size, df, id_column='id', filename_template='{}.wav', temperature=1.0, top_k=DEFAULT_TOP_K, top_p=DEFAULT_TOP_P, do_sample=True, guidance_scale=DEFAULT_GUIDANCE_SCALE, sample_rate=DEFAULT_SAMPLE_RATE, loggers=None)</code>","text":"<p>Generate audio using Accelerate for multi-GPU distribution.</p>"},{"location":"code/tta/#data-utilities","title":"Data Utilities","text":"<p>Simplified TTA dataset loader.</p> <p>Direct dataset loading and tokenization without unnecessary abstractions.</p> <p>Functions:</p> Name Description <code>prepare_dataloader</code> <p>Prepare DataLoader for TTA dataset.</p>"},{"location":"code/tta/#tta.data.prepare_dataloader","title":"<code>prepare_dataloader(cfg, processor)</code>","text":"<p>Prepare DataLoader for TTA dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>Configuration dictionary with dataset and dataloader parameters.</p> required <p>Returns:     DataLoader for the TTA dataset.</p>"},{"location":"code/tta/#tta.data.prepare_dataloader(cfg)","title":"<code>cfg</code>","text":""},{"location":"code/tta/#evaluation","title":"Evaluation","text":"<p>Evaluation metrics for Text-to-Audio generation quality assessment.</p> <p>Classes:</p> Name Description <code>TTAEvaluator</code> <p>Comprehensive evaluator for Text-to-Audio generation.</p>"},{"location":"code/tta/#tta.evaluation.TTAEvaluator","title":"<code>TTAEvaluator(clap_model='laion/clap-htsat-unfused', fad_model='laion/clap-htsat-unfused', device='cuda')</code>","text":"<p>Comprehensive evaluator for Text-to-Audio generation.</p> <p>Initialize evaluator with CLAP and FAD.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>CLAP model identifier</p> <code>'laion/clap-htsat-unfused'</code> <code>str</code> <p>FAD model identifier (defaults to using CLAP embeddings if same)</p> <code>'laion/clap-htsat-unfused'</code> <code>str</code> <p>Device to run models on</p> <code>'cuda'</code> <p>Methods:</p> Name Description <code>evaluate</code> <p>Run comprehensive evaluation.</p>"},{"location":"code/tta/#tta.evaluation.TTAEvaluator(clap_model)","title":"<code>clap_model</code>","text":""},{"location":"code/tta/#tta.evaluation.TTAEvaluator(fad_model)","title":"<code>fad_model</code>","text":""},{"location":"code/tta/#tta.evaluation.TTAEvaluator(device)","title":"<code>device</code>","text":""},{"location":"code/tta/#tta.evaluation.TTAEvaluator.evaluate","title":"<code>evaluate(generated_audio_dir, metadata_path, reference_audio_dir=None, output_dir=None, text_column='caption', filename_column='filename', batch_size=8, compute_fad=True)</code>","text":"<p>Run comprehensive evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>Path</code> <p>Directory with generated audio files</p> required <code>Path</code> <p>Path to CSV with metadata (captions, filenames)</p> required <code>Path | None</code> <p>Directory with reference audio (for FAD)</p> <code>None</code> <code>Path | None</code> <p>Directory to save evaluation results</p> <code>None</code> <code>str</code> <p>Column name for text prompts in metadata</p> <code>'caption'</code> <code>str</code> <p>Column name for audio filenames in metadata</p> <code>'filename'</code> <code>int</code> <p>Batch size for CLAP computation</p> <code>8</code> <code>bool</code> <p>Whether to compute FAD score (requires reference_audio_dir)</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all evaluation metrics</p>"},{"location":"code/tta/#tta.evaluation.TTAEvaluator.evaluate(generated_audio_dir)","title":"<code>generated_audio_dir</code>","text":""},{"location":"code/tta/#tta.evaluation.TTAEvaluator.evaluate(metadata_path)","title":"<code>metadata_path</code>","text":""},{"location":"code/tta/#tta.evaluation.TTAEvaluator.evaluate(reference_audio_dir)","title":"<code>reference_audio_dir</code>","text":""},{"location":"code/tta/#tta.evaluation.TTAEvaluator.evaluate(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"code/tta/#tta.evaluation.TTAEvaluator.evaluate(text_column)","title":"<code>text_column</code>","text":""},{"location":"code/tta/#tta.evaluation.TTAEvaluator.evaluate(filename_column)","title":"<code>filename_column</code>","text":""},{"location":"code/tta/#tta.evaluation.TTAEvaluator.evaluate(batch_size)","title":"<code>batch_size</code>","text":""},{"location":"code/tta/#tta.evaluation.TTAEvaluator.evaluate(compute_fad)","title":"<code>compute_fad</code>","text":""},{"location":"code/utils/","title":"Utilities","text":"<p>Logging and experiment tracking utilities.</p>"},{"location":"code/utils/#rankedlogger","title":"RankedLogger","text":"<p>Distributed-aware logger that only logs from rank 0 process.</p>"},{"location":"code/utils/#utils.pylogger.RankedLogger","title":"<code>utils.pylogger.RankedLogger(name=__name__, rank_zero_only=False, extra=None)</code>","text":"<p>               Bases: <code>LoggerAdapter</code></p> <p>A multi-GPU-friendly python command line logger.</p> <p>Initializes a multi-GPU-friendly python command line logger that logs on all processes with their rank prefixed in the log message.</p> <p>:param name: The name of the logger. Default is <code>__name__</code>. :param rank_zero_only: Whether to force all logs to only occur on the rank zero process. Default is <code>False</code>. :param extra: (Optional) A dict-like object which provides contextual information. See <code>logging.LoggerAdapter</code>.</p> <p>Methods:</p> Name Description <code>log</code> <p>Delegate a log call to the underlying logger, after prefixing its</p>"},{"location":"code/utils/#utils.pylogger.RankedLogger.log","title":"<code>log(level, msg, rank=None, *args, **kwargs)</code>","text":"<p>Delegate a log call to the underlying logger, after prefixing its message with the rank of the process it's being logged from. If <code>'rank'</code> is provided, then the log will only occur on that rank/process.</p> <p>:param level: The level to log at. Look at <code>logging.__init__.py</code> for more information. :param msg: The message to log. :param rank: The rank to log at. :param args: Additional args to pass to the underlying logging function. :param kwargs: Any additional keyword args to pass to the underlying logging function.</p>"},{"location":"code/utils/#experiment-utilities","title":"Experiment Utilities","text":"<p>Functions:</p> Name Description <code>instantiate_callbacks</code> <p>Instantiates callbacks from config.</p> <code>instantiate_loggers</code> <p>Instantiates loggers from config.</p> <code>log_hyperparameters</code> <p>Controls which config parts are saved by Lightning loggers.</p> <code>print_config_tree</code> <p>Prints the contents of a DictConfig as a tree structure using the Rich</p>"},{"location":"code/utils/#utils.logging.instantiate_callbacks","title":"<code>instantiate_callbacks(callbacks_cfg)</code>","text":"<p>Instantiates callbacks from config.</p> <p>:param callbacks_cfg: A DictConfig object containing callback     configurations. :return: A list of instantiated callbacks.</p>"},{"location":"code/utils/#utils.logging.instantiate_loggers","title":"<code>instantiate_loggers(logger_cfg)</code>","text":"<p>Instantiates loggers from config.</p> <p>:param logger_cfg: A DictConfig object containing logger configurations. :return: A list of instantiated loggers.</p>"},{"location":"code/utils/#utils.logging.log_hyperparameters","title":"<code>log_hyperparameters(object_dict)</code>","text":"<p>Controls which config parts are saved by Lightning loggers.</p> Additionally saves <ul> <li>Number of model parameters</li> </ul> <p>:param object_dict: A dictionary containing the following objects:     - <code>\"cfg\"</code>: A DictConfig object containing the main config.     - <code>\"model\"</code>: The Lightning model.     - <code>\"trainer\"</code>: The Lightning trainer.</p>"},{"location":"code/utils/#utils.logging.print_config_tree","title":"<code>print_config_tree(cfg, print_order=('data', 'model', 'callbacks', 'logger', 'trainer', 'paths', 'extras'), resolve=False, save_to_file=False)</code>","text":"<p>Prints the contents of a DictConfig as a tree structure using the Rich library.</p> <p>:param cfg: A DictConfig composed by Hydra. :param print_order: Determines in what order config components are printed. Default is <code>(\"data\", \"model\", \"callbacks\", \"logger\", \"trainer\", \"paths\", \"extras\")</code>. :param resolve: Whether to resolve reference fields of DictConfig. Default is <code>False</code>. :param save_to_file: Whether to export config to the hydra output folder. Default is <code>False</code>.</p>"},{"location":"code/vae/","title":"VAE Module","text":"<p>Beta-VAE for learning plausible music attribute co-occurrence patterns.</p> <p>This module implements a Variational Autoencoder that learns the joint distribution of music attributes from MusicCaps, enabling sampling of realistic attribute combinations.</p>"},{"location":"code/vae/#overview","title":"Overview","text":"<p>The VAE learns which attributes naturally co-occur in music (e.g., \"jazz\" often pairs with \"saxophone\" and \"swing rhythm\"). This prevents generating implausible combinations like \"heavy metal lullaby\".</p> <p>Key insight: Separates semantic modeling (VAE) from text generation (LLM).</p>"},{"location":"code/vae/#configuration","title":"Configuration","text":"<p>Configuration dataclasses for VAE training and inference.</p> <p>Classes:</p> Name Description <code>VAEConfig</code> <code>VAEModelConfig</code> <code>VAEDataConfig</code> <code>VAELossConfig</code>"},{"location":"code/vae/#vae.config.VAEConfig","title":"<code>VAEConfig(model=VAEModelConfig(), data=VAEDataConfig(), trainer=VAETrainerConfig(), loss=VAELossConfig(), paths=VAEPathsConfig(), inference=VAEInferenceConfig(), random_state=42, device='cuda', run_id='${now:%Y-%m-%d_%H-%M-%S}', task_name='vae_training', model_name='vae_final', save_model=True)</code>  <code>dataclass</code>","text":""},{"location":"code/vae/#vae.config.VAEModelConfig","title":"<code>VAEModelConfig(name='MultiLabelVAE', input_dim=0, latent_dim=DEFAULT_LATENT_DIM, hidden_dim=DEFAULT_HIDDEN_DIM, dropout_p=0.3, beta=1.0, use_batch_norm=False)</code>  <code>dataclass</code>","text":""},{"location":"code/vae/#vae.config.VAEDataConfig","title":"<code>VAEDataConfig(taxonomy_path='data/concepts_to_tags.json', dataset_name='google/MusicCaps', dataset_split='train', aspect_column='aspect_list', batch_size=DEFAULT_BATCH_SIZE, dataloader_num_workers=4, shuffle=True, pin_memory=True, persistent_workers=False)</code>  <code>dataclass</code>","text":"<p>Methods:</p> Name Description <code>__post_init__</code> <p>Validate and resolve paths.</p>"},{"location":"code/vae/#vae.config.VAEDataConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate and resolve paths.</p>"},{"location":"code/vae/#vae.config.VAELossConfig","title":"<code>VAELossConfig(bce_weight=1.0, kld_weight=1.0, use_binary_cross_entropy=True)</code>  <code>dataclass</code>","text":""},{"location":"code/vae/#betavae","title":"BetaVAE","text":"<p>The core model architecture with disentanglement via \u03b2-weighted KL divergence.</p>"},{"location":"code/vae/#vae.model.BetaVAE","title":"<code>vae.model.BetaVAE(input_dim, latent_dim=32, hidden_dim=128, dropout_p=0.3, use_batch_norm=False, beta=1.0)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Beta-VAE: Disentangled Variational Autoencoder with weighted KL divergence.</p> <p>Reference: Higgins et al., \"beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\"</p> <p>Methods:</p> Name Description <code>encode</code> <p>Encode input to latent space.</p> <code>decode</code> <p>Decode from latent space to reconstruction.</p> <code>reparameterize</code> <p>Reparameterization trick for sampling from latent space.</p> <code>forward</code> <p>Forward pass through Beta-VAE.</p> <code>sample</code> <p>Generate samples from the prior distribution.</p> <code>reconstruct</code> <p>Reconstruct input without sampling.</p>"},{"location":"code/vae/#vae.model.BetaVAE.encode","title":"<code>encode(x)</code>","text":"<p>Encode input to latent space.</p>"},{"location":"code/vae/#vae.model.BetaVAE.decode","title":"<code>decode(z, temperature=1.0)</code>","text":"<p>Decode from latent space to reconstruction.</p>"},{"location":"code/vae/#vae.model.BetaVAE.reparameterize","title":"<code>reparameterize(mu, logvar)</code>","text":"<p>Reparameterization trick for sampling from latent space.</p>"},{"location":"code/vae/#vae.model.BetaVAE.forward","title":"<code>forward(x, temperature=1.0)</code>","text":"<p>Forward pass through Beta-VAE.</p>"},{"location":"code/vae/#vae.model.BetaVAE.sample","title":"<code>sample(num_samples, device='cpu')</code>","text":"<p>Generate samples from the prior distribution.</p>"},{"location":"code/vae/#vae.model.BetaVAE.reconstruct","title":"<code>reconstruct(x)</code>","text":"<p>Reconstruct input without sampling.</p>"},{"location":"code/vae/#betavaelightningmodule","title":"BetaVAELightningModule","text":"<p>PyTorch Lightning wrapper for training.</p>"},{"location":"code/vae/#vae.lightning_module.BetaVAELightningModule","title":"<code>vae.lightning_module.BetaVAELightningModule(model_cfg, loss_cfg, learning_rate=0.0005, betas=(0.9, 0.999), weight_decay=0.0, beta=1.0)</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Lightning module for training Beta-VAE on multi-label tag data.</p> <p>Beta-VAE uses a weighted KL divergence term to encourage disentangled representations of latent factors.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through Beta-VAE.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> <code>configure_optimizers</code> <p>Configure optimizer and learning rate scheduler.</p>"},{"location":"code/vae/#vae.lightning_module.BetaVAELightningModule.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through Beta-VAE.</p>"},{"location":"code/vae/#vae.lightning_module.BetaVAELightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p>"},{"location":"code/vae/#vae.lightning_module.BetaVAELightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p>"},{"location":"code/vae/#vae.lightning_module.BetaVAELightningModule.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizer and learning rate scheduler.</p>"},{"location":"code/vae/#vaedatamodule","title":"VAEDataModule","text":"<p>Data loading for multi-label tag data.</p>"},{"location":"code/vae/#vae.data.VAEDataModule","title":"<code>vae.data.VAEDataModule(data_cfg, taxonomy_path, batch_size=32, num_workers=4, val_split=0.1, test_split=0.05)</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Lightning DataModule for VAE training on multi-label tag data.</p> <p>Methods:</p> Name Description <code>setup</code> <p>Load and process data.</p> <code>train_dataloader</code> <code>val_dataloader</code> <code>test_dataloader</code>"},{"location":"code/vae/#vae.data.VAEDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Load and process data.</p>"},{"location":"code/vae/#vae.data.VAEDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":""},{"location":"code/vae/#vae.data.VAEDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":""},{"location":"code/vae/#vae.data.VAEDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":""},{"location":"code/vae/#evaluation","title":"Evaluation","text":"<p>Evaluation metrics for VAE training.</p> <p>Classes:</p> Name Description <code>VAEMetrics</code> <p>Compute evaluation metrics for VAE training.</p> <code>MetricsSaver</code> <p>Utility class to save and manage training metrics.</p> <code>MetricsSaveCallback</code> <p>Lightning callback to save metrics at the end of training.</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics","title":"<code>VAEMetrics</code>","text":"<p>Compute evaluation metrics for VAE training.</p> <p>Methods:</p> Name Description <code>hamming_loss</code> <p>Compute Hamming loss: fraction of labels that are incorrectly predicted.</p> <code>jaccard_index</code> <p>Compute Jaccard Index: intersection / union over all samples and labels.</p> <code>active_units</code> <p>Compute active units statistics in the latent space.</p> <code>cluster_separability</code> <p>Compute cluster separability in latent space using different metrics.</p> <code>compute_all_metrics</code> <p>Compute all evaluation metrics.</p> <code>tag_combination_diversity</code> <p>Compute diversity metrics to detect model collapse (generating same</p> <code>tag_cooccurrence_matrix</code> <p>Compute co-occurrence matrix for tag pairs.</p> <code>cooccurrence_comparison</code> <p>Compare co-occurrence matrices between predictions and original data.</p> <code>compute_diversity_metrics</code> <p>Compute all diversity-related metrics.</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics.hamming_loss","title":"<code>hamming_loss(recon, target, threshold=0.5)</code>  <code>staticmethod</code>","text":"<p>Compute Hamming loss: fraction of labels that are incorrectly predicted.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Reconstructed output (batch_size, num_labels)</p> required <code>Tensor</code> <p>Target labels (batch_size, num_labels)</p> required <code>float</code> <p>Threshold for binarizing reconstruction (default: 0.5)</p> <code>0.5</code> <p>Returns:</p> Type Description <code>float</code> <p>Hamming loss (lower is better)</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics.hamming_loss(recon)","title":"<code>recon</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.hamming_loss(target)","title":"<code>target</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.hamming_loss(threshold)","title":"<code>threshold</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.jaccard_index","title":"<code>jaccard_index(recon, target, threshold=0.5)</code>  <code>staticmethod</code>","text":"<p>Compute Jaccard Index: intersection / union over all samples and labels.</p> <p>Also known as Intersection over Union (IoU).</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Reconstructed output (batch_size, num_labels)</p> required <code>Tensor</code> <p>Target labels (batch_size, num_labels)</p> required <code>float</code> <p>Threshold for binarizing reconstruction (default: 0.5)</p> <code>0.5</code> <p>Returns:</p> Type Description <code>float</code> <p>Jaccard Index (higher is better, range [0, 1])</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics.jaccard_index(recon)","title":"<code>recon</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.jaccard_index(target)","title":"<code>target</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.jaccard_index(threshold)","title":"<code>threshold</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.active_units","title":"<code>active_units(latent, threshold=0.0)</code>  <code>staticmethod</code>","text":"<p>Compute active units statistics in the latent space.</p> <p>Active units are latent dimensions that have significant variance across the batch.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Latent representations (batch_size, latent_dim)</p> required <code>float</code> <p>Threshold for standard deviation to consider a unit active (default: 0.0)</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with: - 'num_active_units': Number of active units - 'pct_active_units': Percentage of active units - 'mean_variance': Mean variance across dimensions - 'min_variance': Minimum variance - 'max_variance': Maximum variance</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics.active_units(latent)","title":"<code>latent</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.active_units(threshold)","title":"<code>threshold</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.cluster_separability","title":"<code>cluster_separability(latent, labels, metric='silhouette')</code>  <code>staticmethod</code>","text":"<p>Compute cluster separability in latent space using different metrics.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Latent representations (batch_size, latent_dim)</p> required <code>Tensor</code> <p>Cluster labels (batch_size,) or multi-hot vectors (batch_size, num_labels)</p> required <code>str</code> <p>Metric to use - 'silhouette', 'calinski_harabasz', 'davies_bouldin'</p> <code>'silhouette'</code> <p>Returns:</p> Type Description <code>float</code> <p>Cluster separability score</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics.cluster_separability(latent)","title":"<code>latent</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.cluster_separability(labels)","title":"<code>labels</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.cluster_separability(metric)","title":"<code>metric</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics._silhouette_score","title":"<code>_silhouette_score(X, labels)</code>  <code>staticmethod</code>","text":"<p>Compute Silhouette Coefficient (higher is better, range [-1, 1]).</p> <p>Measures how similar an object is to its own cluster vs other clusters.</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics.compute_all_metrics","title":"<code>compute_all_metrics(recon, target, latent, threshold=0.5)</code>  <code>staticmethod</code>","text":"<p>Compute all evaluation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Reconstructed output</p> required <code>Tensor</code> <p>Target labels</p> required <code>Tensor</code> <p>Latent representations</p> required <code>float</code> <p>Threshold for binarization</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with all computed metrics</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics.compute_all_metrics(recon)","title":"<code>recon</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.compute_all_metrics(target)","title":"<code>target</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.compute_all_metrics(latent)","title":"<code>latent</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.compute_all_metrics(threshold)","title":"<code>threshold</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.tag_combination_diversity","title":"<code>tag_combination_diversity(predictions, threshold=0.5)</code>  <code>staticmethod</code>","text":"<p>Compute diversity metrics to detect model collapse (generating same tag combinations).</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Binary predictions (batch_size, num_tags)</p> required <code>float</code> <p>Threshold for binarization</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with diversity metrics: - 'unique_combinations': Number of unique tag combinations - 'pct_unique_combinations': Percentage of unique combinations - 'entropy': Entropy of combination distribution (higher = more diverse) - 'gini_coefficient': Gini coefficient (0 = uniform, 1 = all in one)</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics.tag_combination_diversity(predictions)","title":"<code>predictions</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.tag_combination_diversity(threshold)","title":"<code>threshold</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.tag_cooccurrence_matrix","title":"<code>tag_cooccurrence_matrix(predictions, threshold=0.5)</code>  <code>staticmethod</code>","text":"<p>Compute co-occurrence matrix for tag pairs.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Binary predictions (batch_size, num_tags)</p> required <code>float</code> <p>Threshold for binarization</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Co-occurrence matrix (num_tags, num_tags) where [i, j] = count of samples with both tag i and j</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics.tag_cooccurrence_matrix(predictions)","title":"<code>predictions</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.tag_cooccurrence_matrix(threshold)","title":"<code>threshold</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.cooccurrence_comparison","title":"<code>cooccurrence_comparison(predicted_cooccurrence, original_cooccurrence)</code>  <code>staticmethod</code>","text":"<p>Compare co-occurrence matrices between predictions and original data.</p> <p>Parameters:</p> Name Type Description Default <code>ndarray</code> <p>Co-occurrence matrix from model predictions</p> required <code>ndarray</code> <p>Co-occurrence matrix from original dataset</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with comparison metrics: - 'cosine_similarity': Cosine similarity between flattened matrices - 'kl_divergence': KL divergence between normalized matrices - 'hellinger_distance': Hellinger distance between distributions - 'pearson_correlation': Pearson correlation between matrices</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics.cooccurrence_comparison(predicted_cooccurrence)","title":"<code>predicted_cooccurrence</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.cooccurrence_comparison(original_cooccurrence)","title":"<code>original_cooccurrence</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.compute_diversity_metrics","title":"<code>compute_diversity_metrics(recon, target, threshold=0.5)</code>  <code>staticmethod</code>","text":"<p>Compute all diversity-related metrics.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>Reconstructed output</p> required <code>Tensor</code> <p>Target labels</p> required <code>float</code> <p>Threshold for binarization</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with all diversity metrics</p>"},{"location":"code/vae/#vae.evaluation.VAEMetrics.compute_diversity_metrics(recon)","title":"<code>recon</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.compute_diversity_metrics(target)","title":"<code>target</code>","text":""},{"location":"code/vae/#vae.evaluation.VAEMetrics.compute_diversity_metrics(threshold)","title":"<code>threshold</code>","text":""},{"location":"code/vae/#vae.evaluation.MetricsSaver","title":"<code>MetricsSaver(output_dir)</code>","text":"<p>Utility class to save and manage training metrics.</p> <p>Initialize MetricsSaver.</p> <p>Parameters:</p> Name Type Description Default <code>Path</code> <p>Directory to save metrics files</p> required <p>Methods:</p> Name Description <code>update</code> <p>Update metrics for a specific stage.</p> <code>save</code> <p>Save metrics to JSON file.</p> <code>save_summary</code> <p>Save metrics summary with configuration.</p>"},{"location":"code/vae/#vae.evaluation.MetricsSaver(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"code/vae/#vae.evaluation.MetricsSaver.update","title":"<code>update(stage, metrics)</code>","text":"<p>Update metrics for a specific stage.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Stage name ('train', 'val', 'test')</p> required <code>Dict[str, float]</code> <p>Dictionary of metric values</p> required"},{"location":"code/vae/#vae.evaluation.MetricsSaver.update(stage)","title":"<code>stage</code>","text":""},{"location":"code/vae/#vae.evaluation.MetricsSaver.update(metrics)","title":"<code>metrics</code>","text":""},{"location":"code/vae/#vae.evaluation.MetricsSaver._clean_metrics","title":"<code>_clean_metrics()</code>","text":"<p>Clean metrics to ensure JSON serializability.</p>"},{"location":"code/vae/#vae.evaluation.MetricsSaver.save","title":"<code>save(filename='metrics.json')</code>","text":"<p>Save metrics to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Name of the metrics file</p> <code>'metrics.json'</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to saved metrics file</p>"},{"location":"code/vae/#vae.evaluation.MetricsSaver.save(filename)","title":"<code>filename</code>","text":""},{"location":"code/vae/#vae.evaluation.MetricsSaver.save_summary","title":"<code>save_summary(config, filename='summary.json')</code>","text":"<p>Save metrics summary with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>Dict[str, Any]</code> <p>Configuration dictionary</p> required <code>str</code> <p>Name of the summary file</p> <code>'summary.json'</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to saved summary file</p>"},{"location":"code/vae/#vae.evaluation.MetricsSaver.save_summary(config)","title":"<code>config</code>","text":""},{"location":"code/vae/#vae.evaluation.MetricsSaver.save_summary(filename)","title":"<code>filename</code>","text":""},{"location":"code/vae/#vae.evaluation.MetricsSaveCallback","title":"<code>MetricsSaveCallback(metrics_saver)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Lightning callback to save metrics at the end of training.</p> <p>Initialize callback.</p> <p>Parameters:</p> Name Type Description Default <code>MetricsSaver</code> <p>MetricsSaver instance</p> required <p>Methods:</p> Name Description <code>on_train_epoch_end</code> <p>Save train metrics at end of each epoch.</p> <code>on_validation_epoch_end</code> <p>Save validation metrics at end of each epoch.</p> <code>on_test_epoch_end</code> <p>Save test metrics at end of testing.</p>"},{"location":"code/vae/#vae.evaluation.MetricsSaveCallback(metrics_saver)","title":"<code>metrics_saver</code>","text":""},{"location":"code/vae/#vae.evaluation.MetricsSaveCallback.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Save train metrics at end of each epoch.</p>"},{"location":"code/vae/#vae.evaluation.MetricsSaveCallback.on_validation_epoch_end","title":"<code>on_validation_epoch_end(trainer, pl_module)</code>","text":"<p>Save validation metrics at end of each epoch.</p>"},{"location":"code/vae/#vae.evaluation.MetricsSaveCallback.on_test_epoch_end","title":"<code>on_test_epoch_end(trainer, pl_module)</code>","text":"<p>Save test metrics at end of testing.</p>"}]}