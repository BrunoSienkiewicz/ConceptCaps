#!/bin/bash
# Comparison of different parallelization strategies for TTA generation

cat << 'EOF'
================================================================================
TTA Generation Parallelization Strategy Comparison
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Strategy 1: SEQUENTIAL (Original)                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ File:           scripts/run_all_tta.sh                                      â”‚
â”‚ GPUs:           1                                                            â”‚
â”‚ Batch Size:     32                                                           â”‚
â”‚ Time:           ~10 hours (all datasets)                                     â”‚
â”‚ Total GPU-hours: 10                                                          â”‚
â”‚                                                                              â”‚
â”‚ Run:            sbatch scripts/run_all_tta.sh                               â”‚
â”‚                                                                              â”‚
â”‚ âœ“ Simple setup                                                               â”‚
â”‚ âœ“ Minimal resources                                                          â”‚
â”‚ âœ— Slowest option                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Strategy 2: MULTI-GPU (New - Best for Single Node)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ File:           scripts/run_all_tta_2gpu.sh                                 â”‚
â”‚ GPUs:           2 (both used per dataset)                                    â”‚
â”‚ Batch Size:     64 (32 per GPU)                                              â”‚
â”‚ Time:           ~5 hours (all datasets)                                      â”‚
â”‚ Total GPU-hours: 10                                                          â”‚
â”‚                                                                              â”‚
â”‚ Run:            sbatch scripts/run_all_tta_2gpu.sh                          â”‚
â”‚                                                                              â”‚
â”‚ âœ“ 2x faster than sequential                                                 â”‚
â”‚ âœ“ Better GPU utilization                                                     â”‚
â”‚ âœ“ Larger effective batch size                                               â”‚
â”‚ âœ“ Same total GPU-hours                                                      â”‚
â”‚ â—‹ Requires 2 GPUs on same node                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Strategy 3: DATASET PARALLELIZATION (New - Best if Many GPUs Available)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ File:           scripts/run_all_tta_parallel.sh (Job Array)                 â”‚
â”‚                 scripts/submit_tta_jobs.sh (Separate Jobs)                  â”‚
â”‚ GPUs:           5 (one per dataset)                                          â”‚
â”‚ Batch Size:     32 per dataset                                               â”‚
â”‚ Time:           ~2 hours (longest dataset)                                   â”‚
â”‚ Total GPU-hours: 10                                                          â”‚
â”‚                                                                              â”‚
â”‚ Run (Array):    sbatch scripts/run_all_tta_parallel.sh                      â”‚
â”‚ Run (Separate): bash scripts/submit_tta_jobs.sh                             â”‚
â”‚                                                                              â”‚
â”‚ âœ“ 5x faster than sequential (if GPUs available)                             â”‚
â”‚ âœ“ Independent jobs - easy to rerun                                          â”‚
â”‚ âœ“ Best utilization if cluster has many GPUs                                 â”‚
â”‚ â—‹ Requires 5 GPUs simultaneously                                            â”‚
â”‚ â—‹ May need to wait in queue                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Strategy 4: HYBRID (Best of Both Worlds)                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Combine multi-GPU + dataset parallelization                                 â”‚
â”‚ GPUs:           10 total (2 per dataset, 5 datasets)                         â”‚
â”‚ Batch Size:     64 per dataset (32 per GPU)                                  â”‚
â”‚ Time:           ~1 hour (longest dataset)                                    â”‚
â”‚ Total GPU-hours: 10                                                          â”‚
â”‚                                                                              â”‚
â”‚ Manual setup required - see instructions below                              â”‚
â”‚                                                                              â”‚
â”‚ âœ“ 10x faster than sequential                                                â”‚
â”‚ âœ“ Maximum throughput                                                         â”‚
â”‚ âœ— Requires 10 GPUs                                                           â”‚
â”‚ âœ— Most complex setup                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

================================================================================
Recommendations
================================================================================

ðŸ“Š Have 2 GPUs available?
   â†’ Use Strategy 2 (Multi-GPU)
   â†’ Command: sbatch scripts/run_all_tta_2gpu.sh
   â†’ Time: ~5 hours

ðŸš€ Have 5+ GPUs available?
   â†’ Use Strategy 3 (Dataset Parallelization)
   â†’ Command: sbatch scripts/run_all_tta_parallel.sh
   â†’ Time: ~2 hours

ðŸ’ª Have 10 GPUs available?
   â†’ Use Strategy 4 (Hybrid) - see setup below
   â†’ Time: ~1 hour

ðŸ’° Limited resources?
   â†’ Use Strategy 1 (Sequential)
   â†’ Command: sbatch scripts/run_all_tta.sh
   â†’ Time: ~10 hours

================================================================================
Hybrid Setup (Strategy 4)
================================================================================

Modify run_all_tta_parallel.sh to use 2 GPUs per task:

1. Edit the array script:
   #SBATCH --gres=gpu:2  # Change from gpu:1 to gpu:2

2. Change the python script call:
   srun python src/scripts/tta/run_tta_generation_parallel.py \\
       +preset=tta/plgrid_musiccaps \\
       data=\$DATASET \\
       data.batch_size=64

3. Submit:
   sbatch scripts/run_all_tta_parallel.sh

This runs 5 array tasks, each with 2 GPUs, processing all datasets in parallel.

================================================================================
Quick Decision Guide
================================================================================

How many GPUs can you allocate RIGHT NOW on your cluster?

  1 GPU  â†’ scripts/run_all_tta.sh (10h)
  2 GPUs â†’ scripts/run_all_tta_2gpu.sh (5h)  â­ RECOMMENDED
  5 GPUs â†’ scripts/run_all_tta_parallel.sh (2h)
  10 GPUs â†’ Hybrid approach (1h)

The 2-GPU approach (Strategy 2) offers the best balance of:
- Speed improvement (2x)
- Resource efficiency
- Setup simplicity
- Availability on most clusters

================================================================================

EOF
