{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "499e07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ffadbb",
   "metadata": {},
   "source": [
    "## 1. Load Data from All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "422d057b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded zero_shot/train: 1000 samples\n",
      "Loaded zero_shot/validation: 100 samples\n",
      "Loaded zero_shot/test: 100 samples\n",
      "Loaded base_random/train: 1000 samples\n",
      "Loaded base_random/validation: 100 samples\n",
      "Loaded base_random/test: 100 samples\n",
      "Loaded base_vae/train: 1000 samples\n",
      "Loaded base_vae/validation: 100 samples\n",
      "Loaded base_vae/test: 100 samples\n",
      "Loaded ft_random/train: 1000 samples\n",
      "Loaded ft_random/validation: 100 samples\n",
      "Loaded ft_random/test: 100 samples\n",
      "Loaded ft_vae/train: 1000 samples\n",
      "Loaded ft_vae/validation: 100 samples\n",
      "Loaded ft_vae/test: 100 samples\n",
      "\n",
      "Loaded data for 5 models across 3 splits\n"
     ]
    }
   ],
   "source": [
    "# Define base directory and model names\n",
    "base_dir = Path(\"../outputs/caption_inference\")\n",
    "models = ['zero_shot', 'base_random', 'base_vae', 'ft_random', 'ft_vae']\n",
    "splits = ['train', 'validation', 'test']\n",
    "\n",
    "# Load all predictions and metrics\n",
    "predictions = {}\n",
    "metrics = {}\n",
    "\n",
    "for model in models:\n",
    "    predictions[model] = {}\n",
    "    metrics[model] = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        # Load predictions\n",
    "        pred_path = base_dir / model / f\"{split}_predictions.csv\"\n",
    "        if pred_path.exists():\n",
    "            predictions[model][split] = pd.read_csv(pred_path)\n",
    "            print(f\"Loaded {model}/{split}: {len(predictions[model][split])} samples\")\n",
    "        \n",
    "        # Load quality metrics\n",
    "        metrics_path = base_dir / model / f\"{split}_quality_metrics.json\"\n",
    "        if metrics_path.exists():\n",
    "            with open(metrics_path, 'r') as f:\n",
    "                metrics[model][split] = json.load(f)\n",
    "\n",
    "print(f\"\\nLoaded data for {len(models)} models across {len(splits)} splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60bd1f",
   "metadata": {},
   "source": [
    "## 2. Filter Empty Samples\n",
    "\n",
    "Remove samples with empty or missing captions/predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "922aaa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_predictions = {}\n",
    "filtering_stats = {}\n",
    "\n",
    "for model in models:\n",
    "    filtered_predictions[model] = {}\n",
    "    filtering_stats[model] = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        if split not in predictions[model]:\n",
    "            continue\n",
    "            \n",
    "        df = predictions[model][split].copy()\n",
    "        original_count = len(df)\n",
    "        \n",
    "        pred_col = 'prediction'\n",
    "        df = df[df[pred_col].notna()]\n",
    "        df = df[df[pred_col].astype(str).str.strip() != '']\n",
    "        df = df[df[pred_col].astype(str).str.lower() != 'nan']\n",
    "        df['prediction_length'] = df[pred_col].astype(str).apply(len)\n",
    "        \n",
    "        filtered_predictions[model][split] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02de8988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged zero_shot: 500 samples\n",
      "Merged base_random: 500 samples\n",
      "Merged base_vae: 500 samples\n",
      "Merged ft_random: 500 samples\n",
      "Merged ft_vae: 500 samples\n"
     ]
    }
   ],
   "source": [
    "# Merge all splits for each model and trim to first 500 samples\n",
    "merged_predictions = {}\n",
    "for model in models:\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for split in splits:\n",
    "        if split in filtered_predictions[model]:\n",
    "            merged_df = pd.concat([merged_df, filtered_predictions[model][split]], ignore_index=True)\n",
    "    \n",
    "    merged_predictions[model] = merged_df.head(500)\n",
    "    print(f\"Merged {model}: {len(merged_predictions[model])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50a9a9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>aspect_list</th>\n",
       "      <th>prediction</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>llm_judge_score</th>\n",
       "      <th>llm_judge_reasoning</th>\n",
       "      <th>prediction_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_0000</td>\n",
       "      <td>punchy kick, happy, passionate, scary, eerie, ...</td>\n",
       "      <td>describe this song.\\n\\nThis up-tempo track is ...</td>\n",
       "      <td>8.472451</td>\n",
       "      <td>9.0</td>\n",
       "      <td>The description accurately incorporates all th...</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_0001</td>\n",
       "      <td>e-bass, fun, hip hop, rhythmic patter</td>\n",
       "      <td>info.\\n\\nThis track starts off with an infecti...</td>\n",
       "      <td>7.610440</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The description is somewhat accurate but lacks...</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_0002</td>\n",
       "      <td>no percussion, punchy kick, electric guitar, v...</td>\n",
       "      <td>:\\n\\nIn this hauntingly beautiful composition,...</td>\n",
       "      <td>14.413651</td>\n",
       "      <td>8.0</td>\n",
       "      <td>The description offers a rich and detailed acc...</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_0003</td>\n",
       "      <td>acoustic drums, shimmering shakers, male voice...</td>\n",
       "      <td>only this:\\n**Song Description**\\n\\nIn \"Slow S...</td>\n",
       "      <td>12.282628</td>\n",
       "      <td>7.0</td>\n",
       "      <td>The description is fairly accurate and coheren...</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample_0004</td>\n",
       "      <td>keyboard accompaniment, shimmering cymbals, ba...</td>\n",
       "      <td>atmospheric.\\n\\nWhat an intriguing combination...</td>\n",
       "      <td>9.046556</td>\n",
       "      <td>6.0</td>\n",
       "      <td>The description attempts to incorporate most o...</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                        aspect_list  \\\n",
       "0  sample_0000  punchy kick, happy, passionate, scary, eerie, ...   \n",
       "1  sample_0001              e-bass, fun, hip hop, rhythmic patter   \n",
       "2  sample_0002  no percussion, punchy kick, electric guitar, v...   \n",
       "3  sample_0003  acoustic drums, shimmering shakers, male voice...   \n",
       "4  sample_0004  keyboard accompaniment, shimmering cymbals, ba...   \n",
       "\n",
       "                                          prediction  perplexity  \\\n",
       "0  describe this song.\\n\\nThis up-tempo track is ...    8.472451   \n",
       "1  info.\\n\\nThis track starts off with an infecti...    7.610440   \n",
       "2  :\\n\\nIn this hauntingly beautiful composition,...   14.413651   \n",
       "3  only this:\\n**Song Description**\\n\\nIn \"Slow S...   12.282628   \n",
       "4  atmospheric.\\n\\nWhat an intriguing combination...    9.046556   \n",
       "\n",
       "   llm_judge_score                                llm_judge_reasoning  \\\n",
       "0              9.0  The description accurately incorporates all th...   \n",
       "1              5.0  The description is somewhat accurate but lacks...   \n",
       "2              8.0  The description offers a rich and detailed acc...   \n",
       "3              7.0  The description is fairly accurate and coheren...   \n",
       "4              6.0  The description attempts to incorporate most o...   \n",
       "\n",
       "   prediction_length  \n",
       "0                634  \n",
       "1                631  \n",
       "2                691  \n",
       "3                667  \n",
       "4                652  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(merged_predictions['base_random'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6caa8547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot sample aspects: ['medium tempo', 'orchestral', 'passionate', 'passionate female vocal', 'punchy kick', 'strings'] caption: This haunting ballad features an intimate piano accompaniment, marked by contemplative arpeggios in C minor, underscoring the emotional weight of the lyrics, which explore themes of longing and separation. A minimalist drum pattern, comprising subtle snare hits and mallet-kissed cymbals, provides understated rhythmic support, evoking a sense of quiet desperation. As the verse progresses, atmospheric synths enter, conjuring ambient textures reminiscent of fog-shrouded landscapes, adding depth and mystery to the narrative. Vocal delivery is characterized by expressive phrasing and emotive vibrato, conveying vulnerability and yearning through\n",
      "base_random aspects: shimmering cymbals, female voice, epic, classical music, electronic, medium tempo caption: extra text.\n",
      "\n",
      "This soaring anthem unfolds like an opera, propelled by shimmering cello-inspired synths, majestic strings, and powerful, driving drums. A classically-trained vocalist delivers a poignant, ethereal performance, her crystalline tones dancing above a bedrock foundation of pulsating basslines and atmospheric pads. Epic arpeggios soar through sweeping orchestral arrangements, while judicious use of reverb and delay heightens the sense of space and grandeur. As the track builds towards its climax, the instrumentation swells, conjuring images of monumental landscapes under starry skies. Shimmering tambourines add subtle texture, complement\n",
      "ft_random aspects: shimmering cymbals, female voice, epic, classical music, electronic, medium tempo caption: This composition contains shimmering cimbols, a sustained strings chord progression and an echoing male voice speaking over it. This is an amateur recording. It has very poor audio quality due to hissing noises. This piece may be playing in a video game movie trailer.\n",
      "base_vae aspects: ['bass', 'heavy metal', 'rock', 'slow tempo'] caption: or extraneous text.\n",
      "\n",
      "The slow-burning heavy metal anthem features crushing guitar riffs delivered through downtuned six-stringed instruments playing palm-muted chords in a hypnotic repetitive pattern, providing an ominous tone reminiscent of dark, foreboding landscapes. A prominent bass line underscores the track's foundation, its pulsing rhythmic figure propelled by driving double bass drumming which underpins every movement forward while also emphasizing certain notes for added emphasis. Melodic interludes emerge amidst this sonic backdrop where soaring vocals soar above the dense arrangement showcasing immense emotional intensity often characterized by growled phrases juxtaposed against clean sung refrains revealing a duality within the\n",
      "ft_vae aspects: ['bass', 'heavy metal', 'rock', 'slow tempo'] caption: This heavy metal song features male vocals singing in an aggressive manner while other male back vocas shout. A bass plays a descending run from high to low pitch in between lines before picking up a groovy line along with distorted electric guitars playing power chords. There is a drum kit playing a double pedal kick pattern with fills. This instrumental has a dark atmosphere. This song can be played in a horror movie scene where the main character sees monsters coming towards him.\n"
     ]
    }
   ],
   "source": [
    "# Find ids present in base_random and ft_random\n",
    "base_random_ids = set(merged_predictions['base_random']['id'])\n",
    "ft_random_ids = set(merged_predictions['ft_random']['id'])\n",
    "common_random_ids = base_random_ids.intersection(ft_random_ids)\n",
    "\n",
    "base_vae_ids = set(merged_predictions['base_vae']['id'])\n",
    "ft_vae_ids = set(merged_predictions['ft_vae']['id'])\n",
    "common_vae_ids = base_vae_ids.intersection(ft_vae_ids)\n",
    "\n",
    "zero_shot_sample = merged_predictions['zero_shot'].iloc[0]\n",
    "print(f\"Zero-shot sample aspects: {zero_shot_sample['aspect_list']} caption: {zero_shot_sample['prediction']}\")\n",
    "\n",
    "# Example caption comparison for every dataset entry\n",
    "sample_id = list(common_random_ids)[1]\n",
    "for model in ['base_random', 'ft_random']:\n",
    "    df = merged_predictions[model]\n",
    "    caption = df[df['id'] == sample_id]['prediction'].values[0]\n",
    "    aspects = df[df['id'] == sample_id]['aspect_list'].values[0]\n",
    "    print(f\"{model} aspects: {aspects} caption: {caption}\")\n",
    "\n",
    "sample_id = list(common_vae_ids)[0]\n",
    "for model in ['base_vae', 'ft_vae']:\n",
    "    df = merged_predictions[model]\n",
    "    caption = df[df['id'] == sample_id]['prediction'].values[0]\n",
    "    aspects = df[df['id'] == sample_id]['aspect_list'].values[0]\n",
    "    print(f\"{model} aspects: {aspects} caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8336559",
   "metadata": {},
   "source": [
    "## Analyze overall statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c2a73b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>avg_prediction_length</th>\n",
       "      <th>avg_perplexity</th>\n",
       "      <th>median_perplexity</th>\n",
       "      <th>avg_llm_judge_score</th>\n",
       "      <th>median_llm_judge_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zero_shot</td>\n",
       "      <td>test</td>\n",
       "      <td>695.348</td>\n",
       "      <td>9.310202</td>\n",
       "      <td>8.858738</td>\n",
       "      <td>5.306</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>base_random</td>\n",
       "      <td>test</td>\n",
       "      <td>660.634</td>\n",
       "      <td>11.850457</td>\n",
       "      <td>10.596377</td>\n",
       "      <td>7.111</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>base_vae</td>\n",
       "      <td>test</td>\n",
       "      <td>666.776</td>\n",
       "      <td>9.975931</td>\n",
       "      <td>9.486908</td>\n",
       "      <td>7.298</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ft_random</td>\n",
       "      <td>test</td>\n",
       "      <td>446.910</td>\n",
       "      <td>13.391273</td>\n",
       "      <td>12.269531</td>\n",
       "      <td>5.769</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ft_vae</td>\n",
       "      <td>test</td>\n",
       "      <td>385.442</td>\n",
       "      <td>14.550156</td>\n",
       "      <td>13.015625</td>\n",
       "      <td>6.717</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model split  avg_prediction_length  avg_perplexity  \\\n",
       "0    zero_shot  test                695.348        9.310202   \n",
       "1  base_random  test                660.634       11.850457   \n",
       "2     base_vae  test                666.776        9.975931   \n",
       "3    ft_random  test                446.910       13.391273   \n",
       "4       ft_vae  test                385.442       14.550156   \n",
       "\n",
       "   median_perplexity  avg_llm_judge_score  median_llm_judge_score  \n",
       "0           8.858738                5.306                     5.0  \n",
       "1          10.596377                7.111                     7.0  \n",
       "2           9.486908                7.298                     7.0  \n",
       "3          12.269531                5.769                     6.0  \n",
       "4          13.015625                6.717                     7.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Table: model, split, avg_prediction_length, avg_perplexity, median_perplexity, avg_llm_judge_score, median_llm_judge_score\n",
    "summary_stats = []\n",
    "\n",
    "for model, df in merged_predictions.items():\n",
    "    avg_pred_length = df['prediction_length'].mean()\n",
    "    avg_perplexity = df['perplexity'].mean()\n",
    "    median_perplexity = df['perplexity'].median()\n",
    "    avg_llm_judge_score = df['llm_judge_score'].mean()\n",
    "    median_llm_judge_score = df['llm_judge_score'].median()\n",
    "    \n",
    "    summary_stats.append({\n",
    "        'model': model,\n",
    "        'split': split,\n",
    "        'avg_prediction_length': avg_pred_length,\n",
    "        'avg_perplexity': avg_perplexity,\n",
    "        'median_perplexity': median_perplexity,\n",
    "        'avg_llm_judge_score': avg_llm_judge_score,\n",
    "        'median_llm_judge_score': median_llm_judge_score\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0883fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot comparing avg perplexity and avg llm_judge_score across zero_shot, base_random, base_vae\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "models_to_plot = ['zero_shot', 'base_random', 'base_vae']\n",
    "x = np.arange(len(models_to_plot))\n",
    "avg_perplexities = [summary_df[summary_df['model'] == model]['avg_perplexity'].values[0] for model in models_to_plot]\n",
    "avg_llm_scores = [\n",
    "    summary_df[summary_df['model'] == model]['avg_llm_judge_score'].values[0] for model in models_to_plot\n",
    "]\n",
    "ax1.bar(x - 0.2, avg_perplexities, width=0.4, label='Avg Perplexity', color='b')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x + 0.2, avg_llm_scores, width=0.4, label='Avg LLM Judge Score', color='g')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models_to_plot)\n",
    "ax1.set_ylabel('Avg Perplexity', color='b')\n",
    "ax2.set_ylabel('Avg LLM Judge Score', color='g')\n",
    "plt.title('Comparison of Avg Perplexity and Avg LLM Judge Score')\n",
    "fig.legend(loc='upper right', bbox_to_anchor=(0.9, 0.9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851686ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot comparing avg prediction length between zero-shot, base vae, and fine-tuned vae models\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "models_to_plot = ['zero_shot', 'base_vae', 'ft_vae']\n",
    "x = np.arange(len(models_to_plot))\n",
    "avg_pred_lengths = [summary_df[summary_df['model'] == model]['avg_prediction_length'].values[0] for model in models_to_plot]\n",
    "ax.bar(x, avg_pred_lengths, color='c')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models_to_plot)\n",
    "ax.set_ylabel('Avg Prediction Length')\n",
    "plt.title('Comparison of Avg Prediction Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "416f730b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2967739bad48b190eb84cd835a6b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff49f77c6e4647e592eab56b87e6e83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e47b9f272141d0bdbfaa1bd22d7de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3c9f6c88684e8ebf18284ec8bea72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6717afe2f8de4614bc2968d376dc3ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/519 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8e4efcb23c4e8b8cbe53cec0732844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74d7ed46a2c4b8da9ed7a7a0ae8a058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a48345df4e484f8d92a02cfc95dd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36ee9871ab942ac83830585f5a302d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b825918479914c4ebaf37d7e1cb1377d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/519 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0702a5cd7beb4869972595b68280f281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a6ef56052d4ab8879fbf2577c82c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334ef4d8c01146f596839add931a0e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480fabe730fe43edb2109d1333f28596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b175f6ff3064eb5a458dc5bd17a431b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/519 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c0ec1ef5a8410e94edab1ac6a8ad3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8487408e810848d98b3384e7d41e2679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485dae7fa7004a67bf39243b129ff72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc535f20ac24cceb2458886ea2086f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f43f60b88a449c88d82fc47abf5154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/519 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e91c35b8cd64d608e2e139265125adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099d6f7c8f2b4abd96efe9afd1e7d304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a3178de4cf44a4a27366b14eab2e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afd2ab016214311ab9c8a72c94a4626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f684f0380f81406290609105e6f34a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/519 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create huggingface datasets for further analysis\n",
    "for model in models:\n",
    "    df = merged_predictions[model]\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    hf_dataset_dict = DatasetDict({ 'test': hf_dataset })\n",
    "    hf_dataset_dict.push_to_hub(f\"bsienkiewicz/{model}-caption-inference-dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
