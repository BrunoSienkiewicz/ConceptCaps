{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d370c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "import ast\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669515d",
   "metadata": {},
   "source": [
    "## Define Tag Categories\n",
    "\n",
    "Define all possible tags for each category based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce64c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXONOMY = json.load(open(\"../data/concepts_to_tags.json\", \"r\"))\n",
    "\n",
    "CATEGORIES = list(TAXONOMY.keys())\n",
    "\n",
    "# Reverse map for easy lookup (tag -> category)\n",
    "TAG_TO_CATEGORY = {}\n",
    "for cat, tags in TAXONOMY.items():\n",
    "    for tag in tags:\n",
    "        TAG_TO_CATEGORY[tag] = cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55f0a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Input Dimension: 200\n"
     ]
    }
   ],
   "source": [
    "tag_to_idx = {}\n",
    "idx_to_tag = {}\n",
    "cat_ranges = {} # Stores start/end index for each category\n",
    "\n",
    "current_idx = 0\n",
    "for cat in CATEGORIES:\n",
    "    start = current_idx\n",
    "    for tag in TAXONOMY[cat]:\n",
    "        tag_to_idx[tag] = current_idx\n",
    "        idx_to_tag[current_idx] = (cat, tag)\n",
    "        current_idx += 1\n",
    "    cat_ranges[cat] = (start, current_idx)\n",
    "\n",
    "TOTAL_INPUT_DIM = current_idx\n",
    "print(f\"Total Input Dimension: {TOTAL_INPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f402e",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ef3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_multilabel(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a Multi-Hot vector for every song.\n",
    "    Example: [0, 1, 0, 1, 1, ...] where 1 means the tag is present.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        raw_tags = ast.literal_eval(row['aspect_list'])\n",
    "        raw_tags = [t.lower() for t in raw_tags]\n",
    "            \n",
    "        # Create Zero Vector\n",
    "        vector = np.zeros(TOTAL_INPUT_DIM, dtype=np.float32)\n",
    "        has_data = False\n",
    "        \n",
    "        for tag in raw_tags:\n",
    "            if tag in tag_to_idx:\n",
    "                idx = tag_to_idx[tag]\n",
    "                vector[idx] = 1.0\n",
    "                has_data = True\n",
    "        \n",
    "        # Only keep records that have at least one valid tag\n",
    "        if has_data:\n",
    "            processed_data.append(vector)\n",
    "            \n",
    "    return np.array(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e64e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"google/MusicCaps\", split=\"train\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab06f01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data shape: (5046, 200)\n"
     ]
    }
   ],
   "source": [
    "data = process_data_multilabel(df)\n",
    "print(f\"Processed data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2362ee6",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ddea9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32, hidden_dim=128):\n",
    "        super(MultiLabelVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim) \n",
    "        \n",
    "        # Dropout for the \"Denoising\" part (applied to input)\n",
    "        self.input_dropout = nn.Dropout(p=0.3) \n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc2_mu(h1), self.fc2_logvar(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, temperature=1.0):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        logits = self.fc4(h3)\n",
    "        return torch.sigmoid(logits / temperature)\n",
    "\n",
    "    def forward(self, x, temperature=1.0):\n",
    "        # Apply dropout to inputs during training -> forces model to learn correlations\n",
    "        x_noisy = self.input_dropout(x)\n",
    "        mu, logvar = self.encode(x_noisy)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z, temperature=temperature)\n",
    "        return recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e303cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization setup\n",
    "input_dim = TOTAL_INPUT_DIM\n",
    "\n",
    "# Define hyperparameter combinations\n",
    "hyperparameter_space = {\n",
    "    'vae_small': {\n",
    "        'latent_dim': 32,\n",
    "        'hidden_dim': 128,\n",
    "        'learning_rate': 5e-3,\n",
    "        'epochs': 300\n",
    "    },\n",
    "    'vae_medium': {\n",
    "        'latent_dim': 64,\n",
    "        'hidden_dim': 256,\n",
    "        'learning_rate': 5e-3,\n",
    "        'epochs': 300\n",
    "    },\n",
    "    'vae_large': {\n",
    "        'latent_dim': 128,\n",
    "        'hidden_dim': 512,\n",
    "        'learning_rate': 5e-3,\n",
    "        'epochs': 300\n",
    "    }\n",
    "}\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef720613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    bce_loss_fn = nn.BCELoss(reduction='sum')\n",
    "    BCE = bce_loss_fn(recon_x, x)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1c1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: vae_small with params: {'latent_dim': 32, 'hidden_dim': 128, 'learning_rate': 0.005, 'epochs': 300}\n",
      "Epoch [50/300], Loss: 14.7081\n",
      "Epoch [50/300], Loss: 14.7081\n",
      "Epoch [100/300], Loss: 14.5597\n",
      "Epoch [100/300], Loss: 14.5597\n",
      "Epoch [150/300], Loss: 14.5648\n",
      "Epoch [150/300], Loss: 14.5648\n",
      "Epoch [200/300], Loss: 14.4779\n",
      "Epoch [200/300], Loss: 14.4779\n",
      "Epoch [250/300], Loss: 14.5191\n",
      "Epoch [250/300], Loss: 14.5191\n",
      "Epoch [300/300], Loss: 14.4475\n",
      "Model vae_small saved.\n",
      "\n",
      "Training model: vae_medium with params: {'latent_dim': 64, 'hidden_dim': 256, 'learning_rate': 0.005, 'epochs': 300}\n",
      "Epoch [300/300], Loss: 14.4475\n",
      "Model vae_small saved.\n",
      "\n",
      "Training model: vae_medium with params: {'latent_dim': 64, 'hidden_dim': 256, 'learning_rate': 0.005, 'epochs': 300}\n",
      "Epoch [50/300], Loss: 14.9363\n",
      "Epoch [50/300], Loss: 14.9363\n",
      "Epoch [100/300], Loss: 14.7020\n",
      "Epoch [100/300], Loss: 14.7020\n",
      "Epoch [150/300], Loss: 14.6514\n",
      "Epoch [150/300], Loss: 14.6514\n",
      "Epoch [200/300], Loss: 14.6524\n",
      "Epoch [200/300], Loss: 14.6524\n",
      "Epoch [250/300], Loss: 14.6190\n",
      "Epoch [250/300], Loss: 14.6190\n",
      "Epoch [300/300], Loss: 14.6011\n",
      "Model vae_medium saved.\n",
      "\n",
      "Training model: vae_large with params: {'latent_dim': 128, 'hidden_dim': 512, 'learning_rate': 0.005, 'epochs': 300}\n",
      "Epoch [300/300], Loss: 14.6011\n",
      "Model vae_medium saved.\n",
      "\n",
      "Training model: vae_large with params: {'latent_dim': 128, 'hidden_dim': 512, 'learning_rate': 0.005, 'epochs': 300}\n",
      "Epoch [50/300], Loss: 15.5325\n",
      "Epoch [50/300], Loss: 15.5325\n",
      "Epoch [100/300], Loss: 15.2073\n",
      "Epoch [100/300], Loss: 15.2073\n",
      "Epoch [150/300], Loss: 15.0216\n",
      "Epoch [150/300], Loss: 15.0216\n",
      "Epoch [200/300], Loss: 15.0152\n",
      "Epoch [200/300], Loss: 15.0152\n",
      "Epoch [250/300], Loss: 14.9571\n",
      "Epoch [250/300], Loss: 14.9571\n",
      "Epoch [300/300], Loss: 14.8416\n",
      "Model vae_large saved.\n",
      "\n",
      "Epoch [300/300], Loss: 14.8416\n",
      "Model vae_large saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, params in hyperparameter_space.items():\n",
    "    print(f\"Training model: {model_name} with params: {params}\")\n",
    "    \n",
    "    latent_dim = params['latent_dim']\n",
    "    hidden_dim = params['hidden_dim']\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['epochs']\n",
    "    \n",
    "    model = MultiLabelVAE(input_dim, latent_dim, hidden_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    data_tensor = torch.tensor(data).to(device)\n",
    "    dataset = torch.utils.data.TensorDataset(data_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(inputs)\n",
    "            loss = vae_loss(recon_batch, inputs, mu, logvar)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(data_tensor)\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), f\"../models/{model_name}.pth\")\n",
    "    print(f\"Model {model_name} saved.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44f84a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/250], Loss: 15.1943\n",
      "Epoch [100/250], Loss: 14.5649\n",
      "Epoch [150/250], Loss: 14.3618\n",
      "Epoch [200/250], Loss: 14.2517\n",
      "Epoch [250/250], Loss: 14.1880\n",
      "Model vae_final saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 32\n",
    "hidden_dim = 128\n",
    "learning_rate = 5e-4\n",
    "num_epochs = 250\n",
    "model_name = \"vae_final\"\n",
    "\n",
    "model = MultiLabelVAE(input_dim, latent_dim, hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "data_tensor = torch.tensor(data).to(device)\n",
    "dataset = torch.utils.data.TensorDataset(data_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(inputs)\n",
    "        loss = vae_loss(recon_batch, inputs, mu, logvar)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_tensor)\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), f\"../models/{model_name}.pth\")\n",
    "print(f\"Model {model_name} saved.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-gen-interpretability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
