{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad965595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "from src.tcav.model import MusicGenreClassifier\n",
    "from src.tcav.tcav import TCAV\n",
    "from src.constants import (\n",
    "    GTZAN_PATH, \n",
    "    TCAV_RESULTS_PATH, \n",
    "    GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH, \n",
    "    GTZAN_GENRES, \n",
    "    GENRE_TO_IDX, \n",
    "    NUM_GENRES,\n",
    "    DEFAULT_NUM_RANDOM_SAMPLES, \n",
    "    DEFAULT_NUM_CONCEPT_SAMPLES, \n",
    "    DEFAULT_NUM_CAV_RUNS\n",
    ")\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (18, 8),\n",
    "    'font.size': 32,\n",
    "    'axes.titlesize': 28,\n",
    "    'axes.labelsize': 25,\n",
    "    'xtick.labelsize': 23,\n",
    "    'ytick.labelsize': 23,\n",
    "    'legend.fontsize': 21\n",
    "})\n",
    "\n",
    "TRAIN_MODEL = False\n",
    "RUN_TCAV_ANALYSIS = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d9ce9",
   "metadata": {},
   "source": [
    "## Define helping variables and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda61786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load concept-to-tags mapping\n",
    "CONCEPTS = json.load(open(\"../data/concepts_to_tags.json\", \"r\"))\n",
    "\n",
    "print(\"Available concept categories:\")\n",
    "for cat, tags in CONCEPTS.items():\n",
    "    print(f\"  {cat}: {len(tags)} tags (e.g., {tags[:3]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55baceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse mapping\n",
    "TAG_TO_CATEGORY = {}\n",
    "for cat, tags in CONCEPTS.items():\n",
    "    for tag in tags:\n",
    "        TAG_TO_CATEGORY[tag] = cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf96090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_array: np.ndarray, sr: int, target_sr: int = 16000, duration: float = 3.0) -> torch.Tensor:\n",
    "    \"\"\"Preprocess audio to fixed length and sample rate.\"\"\"\n",
    "    if sr != target_sr:\n",
    "        audio_array = librosa.resample(audio_array, orig_sr=sr, target_sr=target_sr)\n",
    "    \n",
    "    target_length = int(target_sr * duration)\n",
    "    if len(audio_array) > target_length:\n",
    "        audio_array = audio_array[:target_length]\n",
    "    else:\n",
    "        audio_array = np.pad(audio_array, (0, target_length - len(audio_array)))\n",
    "    \n",
    "    return torch.from_numpy(audio_array).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd2b9d",
   "metadata": {},
   "source": [
    "## Load GTZAN Dataset and Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading GTZAN dataset from local files...\")\n",
    "\n",
    "print(\"Scanning audio files...\")\n",
    "audio_files = []\n",
    "for genre in GTZAN_GENRES:\n",
    "    genre_path = GTZAN_PATH / genre\n",
    "    wav_files = sorted(genre_path.glob(\"*.wav\"))\n",
    "    for wav_file in wav_files:\n",
    "        audio_files.append({\n",
    "            'path': wav_file,\n",
    "            'genre': genre,\n",
    "            'label': GENRE_TO_IDX[genre]\n",
    "        })\n",
    "\n",
    "print(f\"Loaded {len(audio_files)} audio files\")\n",
    "print(f\"Genres: {GTZAN_GENRES}\")\n",
    "print(f\"Files per genre: ~{len(audio_files) // len(GTZAN_GENRES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading and preprocessing GTZAN audio files...\")\n",
    "X_train, y_train = [], []\n",
    "\n",
    "for file_info in tqdm(audio_files, desc=\"Loading audio\"):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_info['path'], sr=None, mono=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_info['path']}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    audio_tensor = preprocess_audio(audio, sr)\n",
    "    X_train.append(audio_tensor)\n",
    "    y_train.append(file_info['label'])\n",
    "\n",
    "X_train = torch.stack(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Labels shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicGenreClassifier(num_genres=NUM_GENRES)\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acc4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Training mode: {'ENABLED' if TRAIN_MODEL else 'DISABLED'}\")\n",
    "print(f\"Model checkpoint path: {GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    print(\"Training genre classifier on GTZAN...\")\n",
    "    \n",
    "    indices = torch.randperm(len(X_train))\n",
    "    train_size = int(0.8 * len(X_train))\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    X_train_split = X_train[train_indices]\n",
    "    y_train_split = y_train[train_indices]\n",
    "    X_val = X_train[val_indices]\n",
    "    y_val = y_train[val_indices]\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_split, y_train_split)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        dirpath=GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH.parent,\n",
    "        filename='genre_classifier_best',\n",
    "        monitor='val_acc',\n",
    "        mode='max',\n",
    "        save_top_k=1,\n",
    "        save_last=True\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            pl.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=20,\n",
    "                mode='min'\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    print(f\"Model training complete!\")\n",
    "    print(f\"Best model saved to: {checkpoint_callback.best_model_path}\")\n",
    "    \n",
    "    model = MusicGenreClassifier.load_from_checkpoint(\n",
    "        checkpoint_callback.best_model_path,\n",
    "        num_genres=len(GTZAN_GENRES)\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"Loading pre-trained model from checkpoint...\")\n",
    "    \n",
    "    if GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH.exists():\n",
    "        model = MusicGenreClassifier.load_from_checkpoint(\n",
    "            GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH,\n",
    "            num_genres=len(GTZAN_GENRES)\n",
    "        )\n",
    "        print(f\"Model loaded from: {GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH}\")\n",
    "    else:\n",
    "        print(f\"Checkpoint not found at {GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH}\")\n",
    "        print(\"Please set TRAIN_MODEL=True to train a new model first.\")\n",
    "        raise FileNotFoundError(f\"Model checkpoint not found: {GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH}\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model ready on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4eb6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model accuracy on GTZAN validation set...\")\n",
    "val_dataset = TensorDataset(X_train, y_train)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs, return_bottleneck=False)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy on GTZAN: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12730c",
   "metadata": {},
   "source": [
    "## Load ConceptCaps Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d35e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptcaps = load_dataset(\"bsienkiewicz/ConceptCaps\", \"25pct-audio\", split=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_by_tags(tag: str, num_samples: int) -> List[torch.Tensor]:\n",
    "    samples = []\n",
    "    \n",
    "    for idx, sample in enumerate(conceptcaps):\n",
    "        all_aspects = (\n",
    "            sample['genre_aspects'] + \n",
    "            sample['mood_aspects'] + \n",
    "            sample['instrument_aspects'] + \n",
    "            sample['tempo_aspects']\n",
    "        )\n",
    "        \n",
    "        if tag in all_aspects:\n",
    "            try:\n",
    "                audio_data = sample['file_name']\n",
    "                audio_array = audio_data['array']\n",
    "                sr = audio_data['sampling_rate']\n",
    "\n",
    "                audio_tensor = preprocess_audio(audio_array, sr)\n",
    "                samples.append(audio_tensor)\n",
    "                \n",
    "                if len(samples) >= num_samples:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading audio for sample {idx}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if num_samples > len(samples):\n",
    "        print(f\"Warning: Found {len(samples)}/{num_samples} samples.\")\n",
    "\n",
    "    return samples[:num_samples]\n",
    "\n",
    "\n",
    "def create_concept_dataset_from_audio(concept: str, num_samples: int = 20) -> torch.Tensor:\n",
    "    samples = get_audio_by_tags(concept, num_samples)\n",
    "    return torch.stack(samples)\n",
    "\n",
    "\n",
    "def create_random_audio_dataset(num_samples: int = 30) -> torch.Tensor:\n",
    "    samples = []\n",
    "    indices = np.random.choice(len(conceptcaps), num_samples, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        try:\n",
    "            sample = conceptcaps[int(idx)]\n",
    "            audio_data = sample['file_name']\n",
    "            audio_array = audio_data['array']\n",
    "            sr = audio_data['sampling_rate']\n",
    "\n",
    "            audio_tensor = preprocess_audio(audio_array, sr)\n",
    "            samples.append(audio_tensor)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio for sample {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return torch.stack(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25f080c",
   "metadata": {},
   "source": [
    "## Run TCAV Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25048a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcav = TCAV(model, device)\n",
    "print(\"TCAV analyzer initialized with improved implementation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects_by_category = {\n",
    "    'instrument': set(),\n",
    "    'tempo': set()\n",
    "}\n",
    "\n",
    "for sample in conceptcaps:\n",
    "    aspects_by_category['instrument'].update(sample['instrument_aspects'])\n",
    "    aspects_by_category['tempo'].update(sample['tempo_aspects'])\n",
    "\n",
    "ANALYSIS_CONCEPTS = {\n",
    "    'tempo': [],\n",
    "    'instrument': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a786d",
   "metadata": {},
   "source": [
    "### Select only the most popular concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in ANALYSIS_CONCEPTS:\n",
    "    concept_counts = defaultdict(int)\n",
    "    for sample in conceptcaps:\n",
    "        if category == 'tempo':\n",
    "            for aspect in sample['tempo_aspects']:\n",
    "                concept_counts[aspect] += 1\n",
    "        elif category == 'instrument':\n",
    "            for aspect in sample['instrument_aspects']:\n",
    "                concept_counts[aspect] += 1\n",
    "    sorted_concepts = sorted(concept_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_concepts = [concept for concept, count in sorted_concepts[:15]]\n",
    "    ANALYSIS_CONCEPTS[category] = top_concepts\n",
    "print(\"Concepts selected for TCAV analysis:\")\n",
    "for cat, concepts in ANALYSIS_CONCEPTS.items():\n",
    "    print(f\"  {cat}: {concepts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434bf92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TCAV_ANALYSIS:\n",
    "    random_data = create_random_audio_dataset(num_samples=DEFAULT_NUM_RANDOM_SAMPLES)\n",
    "    random_acts = tcav.get_activations(random_data)\n",
    "\n",
    "    results = defaultdict(dict)\n",
    "    for category, concept_list in ANALYSIS_CONCEPTS.items():\n",
    "        print(f\"Category: {category.upper()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for concept in concept_list:\n",
    "            print(f\"  Loading audio for '{concept}'...\")\n",
    "            concept_data = create_concept_dataset_from_audio(concept, num_samples=DEFAULT_NUM_CONCEPT_SAMPLES)\n",
    "\n",
    "            if concept_data.shape[0] < DEFAULT_NUM_CONCEPT_SAMPLES:\n",
    "                print(f\"  Skipping concept '{concept}' due to insufficient samples.\")\n",
    "                continue\n",
    "\n",
    "            concept_acts = tcav.get_activations(concept_data)\n",
    "            \n",
    "            cav_result = tcav.train_cav(concept_acts, random_acts, num_runs=DEFAULT_NUM_CAV_RUNS)\n",
    "\n",
    "            if cav_result['accuracy'] < 0.65:\n",
    "                print(f\"  Skipping concept '{concept}' due to low CAV accuracy ({cav_result['accuracy']:.3f}).\")\n",
    "                continue\n",
    "            \n",
    "            genre_scores = {}\n",
    "            for genre_name in GTZAN_GENRES:\n",
    "                genre_idx = GENRE_TO_IDX[genre_name]\n",
    "                genre_mask = y_train == genre_idx\n",
    "                genre_samples = X_train[genre_mask]\n",
    "                \n",
    "                genre_acts = tcav.get_activations(genre_samples)\n",
    "                genre_scores[genre_name] = tcav.compute_tcav_score(genre_acts, cav_result['cav'])\n",
    "            \n",
    "            results[category][concept] = {\n",
    "                'cav_accuracy': cav_result['accuracy'],\n",
    "                'genre_scores': genre_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"  {concept}: CAV acc={cav_result['accuracy']:.3f}\")\n",
    "\n",
    "    with open(TCAV_RESULTS_PATH, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"TCAV results saved to: {TCAV_RESULTS_PATH}\")\n",
    "else:\n",
    "    with open(TCAV_RESULTS_PATH, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"TCAV results loaded from: {TCAV_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d5330",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_concepts = [c for concepts in ANALYSIS_CONCEPTS.values() for c in concepts]\n",
    "\n",
    "score_matrix = []\n",
    "concept_names = []\n",
    "\n",
    "for category, concept_dict in results.items():\n",
    "    for concept, data in concept_dict.items():\n",
    "        scores = [data['genre_scores'].get(g, 0.0) for g in GTZAN_GENRES]\n",
    "        score_matrix.append(scores)\n",
    "        concept_names.append(f\"{concept}\")\n",
    "\n",
    "score_matrix = np.array(score_matrix)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 24))\n",
    "sns.heatmap(\n",
    "    score_matrix, \n",
    "    xticklabels=GTZAN_GENRES,\n",
    "    yticklabels=concept_names,\n",
    "    annot=True, \n",
    "    fmt='.2f',\n",
    "    vmin=0, vmax=1,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title('TCAV Scores: Concept Importance per Genre', fontweight='bold')\n",
    "ax.set_xlabel('Genre')\n",
    "ax.set_ylabel('Concept')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07079017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "PLOT_GENRES = ['blues', 'country', 'jazz','classical', 'pop', 'metal', 'rock']\n",
    "\n",
    "PLOT_CONCEPTS = {\n",
    "    'tempo': ['slow tempo', 'moderate tempo', 'fast tempo'],\n",
    "    'instrument': ['electric guitar', 'acoustic guitar', 'percussion']\n",
    "}\n",
    "\n",
    "all_plot_concepts = [c for cat_list in PLOT_CONCEPTS.values() for c in cat_list]\n",
    "unique_concepts = sorted(list(set(all_plot_concepts)))\n",
    "petroff_colors = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "color_map = {concept: color for concept, color in zip(unique_concepts, petroff_colors)}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bar_width = 0.12\n",
    "x_indices = np.arange(len(PLOT_GENRES))\n",
    "\n",
    "for i, genre in enumerate(PLOT_GENRES):\n",
    "    genre_data = []\n",
    "    \n",
    "    for category, concept_dict in results.items():\n",
    "        if category not in PLOT_CONCEPTS: continue\n",
    "        for concept, data in concept_dict.items():\n",
    "            if concept not in PLOT_CONCEPTS[category]: continue\n",
    "            \n",
    "            score = data['genre_scores'].get(genre, 0.0)\n",
    "            genre_data.append((concept, score))\n",
    "    \n",
    "    genre_data.sort(key=lambda x: x[1])\n",
    "    n_bars = len(genre_data)\n",
    "    total_group_width = n_bars * bar_width\n",
    "    start_x = i - (total_group_width / 2) + (bar_width / 2)\n",
    "    \n",
    "    for j, (concept, score) in enumerate(genre_data):\n",
    "        pos = start_x + (j * bar_width)\n",
    "        ax.bar(pos, score, width=bar_width, edgecolor='black', linewidth=1.2, color=color_map[concept], alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x_indices)\n",
    "ax.set_xticklabels([g.upper() for g in PLOT_GENRES], fontweight='bold')\n",
    "ax.set_ylabel('TCAV Score', fontweight='bold')\n",
    "ax.set_title('Concept Influence on Genre Classification\\n(Higher â†’ more important)', fontweight='bold', pad=20)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=color_map[c], label=c) for c in unique_concepts]\n",
    "ax.legend(handles=legend_patches, \n",
    "          title='Concepts', \n",
    "          bbox_to_anchor=(1.01, 1), \n",
    "          loc='upper left',\n",
    "          frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../docs/assets/concept_influence_across_genres.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "accuracies = []\n",
    "labels = []\n",
    "colors = []\n",
    "color_map = {'tempo': 'steelblue', 'instrument': 'coral', 'mood': 'forestgreen'}\n",
    "\n",
    "for category, concept_dict in results.items():\n",
    "    for concept, data in concept_dict.items():\n",
    "        accuracies.append(data['cav_accuracy'])\n",
    "        labels.append(concept)\n",
    "        colors.append(color_map.get(category, 'gray'))\n",
    "\n",
    "bars = ax.bar(range(len(accuracies)), accuracies, color=colors, alpha=0.8)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.set_ylabel('CAV Classifier Accuracy')\n",
    "ax.set_title('CAV Training Accuracy per Concept', fontweight='bold')\n",
    "ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, label=cat) for cat, c in color_map.items()]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptcaps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
