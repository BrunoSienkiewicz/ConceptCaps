{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_context('notebook', font_scale=1.4)\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (18, 8),\n",
    "    'font.size': 32,\n",
    "    'axes.titlesize': 28,\n",
    "    'axes.labelsize': 25,\n",
    "    'xtick.labelsize': 23,\n",
    "    'ytick.labelsize': 23,\n",
    "    'legend.fontsize': 21\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ffadbb",
   "metadata": {},
   "source": [
    "## Ablation study of generated captions\n",
    "\n",
    "We performed comparison between different LLM conditioning methods to show improvements in our methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../data/evaluation_results\")\n",
    "models = ['ft', 'base', 'zero_shot']\n",
    "predictions = {}\n",
    "\n",
    "for model in models:\n",
    "    predictions[model] = {}\n",
    "    pred_path = base_dir / f\"{model}.csv\"\n",
    "    if pred_path.exists():\n",
    "        predictions[model] = pd.read_csv(pred_path)\n",
    "\n",
    "print(f\"\\nLoaded data for {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random id present in all models\n",
    "common_ids = set.intersection(*(set(predictions[model]['id']) for model in models))\n",
    "sample_id = np.random.choice(list(common_ids))\n",
    "print(f\"\\nSample ID: {sample_id}\\n\")\n",
    "for model in models:\n",
    "    caption = predictions[model].loc[predictions[model]['id'] == sample_id, 'prediction'].values[0]\n",
    "    aspects = predictions[model].loc[predictions[model]['id'] == sample_id, 'aspect_list'].values[0]\n",
    "    print(f\"{model}\\n   aspects: {aspects}\\n   caption: {caption}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc978de",
   "metadata": {},
   "source": [
    "### Create huggingface datasets for audio inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    df = predictions[model]\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    hf_dataset_dict = DatasetDict({ 'test': hf_dataset })\n",
    "    hf_dataset_dict.push_to_hub(f\"bsienkiewicz/{model}-caption-inference-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceebb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_test = pd.read_csv(base_dir / \"quick_test.csv\")\n",
    "hf_dataset = Dataset.from_pandas(quick_test)\n",
    "hf_dataset_dict = DatasetDict({ 'test': hf_dataset })\n",
    "hf_dataset_dict.push_to_hub(\"bsienkiewicz/quick-test-caption-inference-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb004a2e",
   "metadata": {},
   "source": [
    "## Analyse ConceptCaps final captions\n",
    "\n",
    "We used imprved prompt that further condensed generated caption to better match MusicCaps descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d25593",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dir = Path(\"../data/generated_captions/\")\n",
    "splits = ['train', 'validation', 'test']\n",
    "captions = {}\n",
    "datasets = {}\n",
    "\n",
    "for split in splits:\n",
    "    pred_path = final_dir / f\"final_{split}.csv\"\n",
    "    df = pd.read_csv(pred_path)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    captions[split] = df\n",
    "    datasets[split] = dataset\n",
    "    print(f\"Loaded {split}: {len(df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf303b0d",
   "metadata": {},
   "source": [
    "### Load MusicCaps for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_dataset = load_dataset(\"google/MusicCaps\", split=\"train\")\n",
    "mc_df = mc_dataset.to_pandas()\n",
    "captions['mc'] = mc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "comparison_data = []\n",
    "datasets = ['train', 'mc']\n",
    "dataset_to_name = {\n",
    "    'train': 'Fine-tuned Train',\n",
    "    'mc': 'MusicCaps'\n",
    "}\n",
    "\n",
    "petroff_colors = [\"#3f90da\", \"#bd1f01\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = captions[dataset]\n",
    "    col_name = 'prediction' if 'prediction' in df.columns else 'caption'\n",
    "    df[col_name] = df[col_name].astype(str)\n",
    "    \n",
    "    word_counts = df[col_name].apply(lambda x: len(x.split()))\n",
    "    char_counts = df[col_name].apply(len)\n",
    "    \n",
    "    # Create temporary dataframe for plotting\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Dataset': dataset_to_name[dataset],\n",
    "        'Word Count': word_counts,\n",
    "        'Character Count': char_counts\n",
    "    })\n",
    "    comparison_data.append(temp_df)\n",
    "\n",
    "viz_df = pd.concat(comparison_data, ignore_index=True)\n",
    "\n",
    "# Split data\n",
    "ft_df = viz_df[viz_df['Dataset'] != 'MusicCaps']\n",
    "mc_df = viz_df[viz_df['Dataset'] == 'MusicCaps']\n",
    "\n",
    "# Define consistent bins for both datasets\n",
    "char_bins = np.linspace(viz_df['Character Count'].min(), viz_df['Character Count'].max(), 40)\n",
    "word_bins = np.linspace(viz_df['Word Count'].min(), viz_df['Word Count'].max(), 40)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(22, 8))\n",
    "\n",
    "# Character counts with dual y-axis\n",
    "ax0_twin = axes[0].twinx()\n",
    "sns.histplot(\n",
    "    data=ft_df, \n",
    "    x='Character Count', \n",
    "    kde=True, \n",
    "    element='step', \n",
    "    stat='density', \n",
    "    common_norm=False,\n",
    "    alpha=0.4, \n",
    "    ax=axes[0],\n",
    "    bins=char_bins,\n",
    "    color=petroff_colors[0]\n",
    ")\n",
    "sns.histplot(\n",
    "    data=mc_df, \n",
    "    x='Character Count', \n",
    "    kde=True, \n",
    "    element='step', \n",
    "    stat='density', \n",
    "    alpha=0.4,\n",
    "    ax=ax0_twin,\n",
    "    bins=char_bins,\n",
    "    label='MusicCaps',\n",
    "    color=petroff_colors[1]\n",
    ")\n",
    "axes[0].set_title('Distribution of Character Counts', fontweight='bold')\n",
    "axes[0].set_xlabel('Character Count', fontweight='bold')\n",
    "axes[0].set_ylabel('Density', fontweight='bold')\n",
    "ax0_twin.set_ylabel('')\n",
    "\n",
    "# Word counts with dual y-axis\n",
    "ax1_twin = axes[1].twinx()\n",
    "sns.histplot(\n",
    "    data=ft_df, \n",
    "    x='Word Count', \n",
    "    kde=False, \n",
    "    element='step', \n",
    "    stat='density', \n",
    "    common_norm=False,\n",
    "    alpha=0.4,\n",
    "    ax=axes[1],\n",
    "    bins=word_bins,\n",
    "    color=petroff_colors[0]\n",
    ")\n",
    "sns.histplot(\n",
    "    data=mc_df, \n",
    "    x='Word Count', \n",
    "    kde=True, \n",
    "    element='step', \n",
    "    stat='density', \n",
    "    alpha=0.4,\n",
    "    ax=ax1_twin,\n",
    "    bins=word_bins,\n",
    "    label='MusicCaps',\n",
    "    color=petroff_colors[1]\n",
    ")\n",
    "axes[1].set_title('Distribution of Word Counts', fontweight='bold')\n",
    "axes[1].set_xlabel('Word Count', fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "ax1_twin.set_ylabel('')\n",
    "\n",
    "# Disable y tick values\n",
    "ax0_twin.set_yticks([])\n",
    "axes[0].set_yticks([])\n",
    "ax1_twin.set_yticks([])\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "legend_elems = [Patch(facecolor=petroff_colors[1], edgecolor='black', label='MusicCaps', alpha=0.4),\n",
    "                Patch(facecolor=petroff_colors[0], edgecolor='black', label='ConceptCaps', alpha=0.4)]\n",
    "fig.legend(handles=legend_elems, loc='upper center', ncol=2, framealpha=0.9, bbox_to_anchor=(0.5, 1.05))\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Caption Length Comparison Between\\nConceptCaps and MusicCaps Dataset', fontweight='bold', y=1.25)\n",
    "plt.style.use('petroff10')\n",
    "plt.savefig(\"../docs/assets/caption_length_comparison.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee910b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, df in captions.items():\n",
    "    if split == 'mc':\n",
    "        continue\n",
    "    print(f\"{split} - Avg Prediction Length: {df['prediction'].astype(str).apply(len).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd638a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, df in captions.items():\n",
    "    if split == 'mc':\n",
    "        continue\n",
    "    df['word_count'] = df['prediction'].astype(str).apply(lambda x: len(x.split()))\n",
    "    print(f\"{split} - Avg Word Count: {df['word_count'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset_dict = DatasetDict({ split: datasets[split] for split in splits })\n",
    "hf_dataset_dict.push_to_hub(\"bsienkiewicz/final-caption-inference-dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptcaps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
