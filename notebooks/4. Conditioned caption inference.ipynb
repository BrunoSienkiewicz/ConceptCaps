{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499e07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ffadbb",
   "metadata": {},
   "source": [
    "## Load Data from All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422d057b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded data for 3 models\n"
     ]
    }
   ],
   "source": [
    "# Define base directory and model names\n",
    "base_dir = Path(\"../data/generated_captions\")\n",
    "models = ['ft', 'base', 'zero_shot']\n",
    "\n",
    "# Load all predictions and metrics\n",
    "predictions = {}\n",
    "metrics = {}\n",
    "\n",
    "for model in models:\n",
    "    predictions[model] = {}\n",
    "    metrics[model] = {}\n",
    "    \n",
    "    # Load predictions\n",
    "    pred_path = base_dir / f\"{model}.csv\"\n",
    "    if pred_path.exists():\n",
    "        predictions[model] = pd.read_csv(pred_path)\n",
    "\n",
    "print(f\"\\nLoaded data for {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(predictions['zero_shot'].head())\n",
    "display(predictions['base'].head())\n",
    "display(predictions['ft'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28611ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the same aspects list across models\n",
    "for model in ['zero_shot', 'base', 'ft']:\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    print(\"Aspects:\", predictions[model]['aspect_list'].iloc[0])\n",
    "    print(\"Caption:\", predictions[model]['prediction'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8336559",
   "metadata": {},
   "source": [
    "## Analyze overall statistics and compare to MusicCaps as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mc_dataset = load_dataset(\"google/MusicCaps\", split=\"train\")\n",
    "mc_df = mc_dataset.to_pandas()\n",
    "predictions['mc'] = mc_df\n",
    "mc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for comparison\n",
    "comparison_data = []\n",
    "models_to_compare = ['zero_shot', 'base', 'ft', 'mc']\n",
    "model_labels = {\n",
    "    'zero_shot': 'Zero Shot', \n",
    "    'base': 'Base LLM', \n",
    "    'ft': 'Fine-tuned LLM',\n",
    "    'mc': 'MusicCaps Dataset'\n",
    "}\n",
    "\n",
    "for model in models_to_compare:\n",
    "    df = predictions[model]\n",
    "    col_name = 'prediction' if 'prediction' in df.columns else 'caption'\n",
    "    df[col_name] = df[col_name].astype(str)\n",
    "    \n",
    "    word_counts = df[col_name].apply(lambda x: len(x.split()))\n",
    "    char_counts = df[col_name].apply(len)\n",
    "    \n",
    "    # Create temporary dataframe for plotting\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Model': model_labels.get(model, model),\n",
    "        'Word Count': word_counts,\n",
    "        'Character Count': char_counts\n",
    "    })\n",
    "    comparison_data.append(temp_df)\n",
    "\n",
    "viz_df = pd.concat(comparison_data, ignore_index=True)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plt.style.use('petroff10')\n",
    "\n",
    "sns.histplot(\n",
    "    data=viz_df, \n",
    "    x='Character Count', \n",
    "    hue='Model', \n",
    "    kde=True, \n",
    "    element='step', \n",
    "    stat='density', \n",
    "    common_norm=False,\n",
    "    alpha=0.3, \n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Distribution of Character Counts')\n",
    "axes[0].set_xlabel('Character Count')\n",
    "\n",
    "sns.histplot(\n",
    "    data=viz_df, \n",
    "    x='Word Count', \n",
    "    hue='Model', \n",
    "    kde=True, \n",
    "    element='step', \n",
    "    stat='density', \n",
    "    common_norm=False,\n",
    "    alpha=0.3,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('Distribution of Word Counts')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc978de",
   "metadata": {},
   "source": [
    "### Create huggingface datasets for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    df = predictions[model]['test']\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    hf_dataset_dict = DatasetDict({ 'test': hf_dataset })\n",
    "    hf_dataset_dict.push_to_hub(f\"bsienkiewicz/{model}-caption-inference-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceebb5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2df5b9c70c4e4da32ca83ee45df6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d96f4bbfb7d4770a0c6f9b84507fb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9988e165afbd4632ab6e00cbd90161ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf7f345ac664a0cb853a657fe0f9d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/bsienkiewicz/quick-test-caption-inference-dataset/commit/2a94a7780103202bfa00399d901ef3f57eeb5e41', commit_message='Upload dataset', commit_description='', oid='2a94a7780103202bfa00399d901ef3f57eeb5e41', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/bsienkiewicz/quick-test-caption-inference-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='bsienkiewicz/quick-test-caption-inference-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_test = pd.read_csv(base_dir / \"quick_test.csv\")\n",
    "hf_dataset = Dataset.from_pandas(quick_test)\n",
    "hf_dataset_dict = DatasetDict({ 'test': hf_dataset })\n",
    "hf_dataset_dict.push_to_hub(\"bsienkiewicz/quick-test-caption-inference-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb004a2e",
   "metadata": {},
   "source": [
    "## Push ConceptCaps dataset captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d25593",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dir = Path(\"../outputs/caption_inference/final\")\n",
    "splits = ['train', 'validation', 'test']\n",
    "predictions = {}\n",
    "datasets = {}\n",
    "\n",
    "for split in splits:\n",
    "    # Load predictions\n",
    "    pred_path = final_dir / f\"{split}_predictions.csv\"\n",
    "    df = pd.read_csv(pred_path)\n",
    "    display(df.head())\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    predictions[split] = df\n",
    "    datasets[split] = dataset\n",
    "    print(f\"Loaded {split}: {len(df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee910b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analuze final inference predictions\n",
    "for split, df in predictions.items():\n",
    "    print(f\"{split} - Avg Prediction Length: {df['prediction'].astype(str).apply(len).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd638a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze word amount in captions\n",
    "for split, df in predictions.items():\n",
    "    df['word_count'] = df['prediction'].astype(str).apply(lambda x: len(x.split()))\n",
    "    print(f\"{split} - Avg Word Count: {df['word_count'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of prediction lengths for final inference predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "for split, df in predictions.items():\n",
    "    sns.kdeplot(df['prediction'].astype(str).apply(len), label=split)\n",
    "plt.title('Distribution of Prediction Lengths for Final Inference Predictions')\n",
    "plt.xlabel('Prediction Length')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation between prediction length and amount of aspects\n",
    "import ast\n",
    "\n",
    "for split, df in predictions.items():\n",
    "    df['num_aspects'] = df['aspect_list'].map(ast.literal_eval).apply(len)\n",
    "    correlation = df['prediction'].astype(str).apply(len).corr(df['num_aspects'])\n",
    "    print(f\"{split} - Correlation between Prediction Length and Number of Aspects: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset_dict = DatasetDict({ split: datasets[split] for split in splits })\n",
    "hf_dataset_dict.push_to_hub(\"bsienkiewicz/ConceptCaps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concept-caps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
