{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad965595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d9ce9",
   "metadata": {},
   "source": [
    "## 1. Load Custom Concept Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda61786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load concept-to-tags mapping\n",
    "CONCEPTS = json.load(open(\"../data/concepts_to_tags.json\", \"r\"))\n",
    "\n",
    "print(\"Available concept categories:\")\n",
    "for cat, tags in CONCEPTS.items():\n",
    "    print(f\"  {cat}: {len(tags)} tags (e.g., {tags[:3]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55baceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse mapping\n",
    "TAG_TO_CATEGORY = {}\n",
    "for cat, tags in CONCEPTS.items():\n",
    "    for tag in tags:\n",
    "        TAG_TO_CATEGORY[tag] = cat\n",
    "\n",
    "# Define target genres for analysis\n",
    "TARGET_GENRES = ['rock', 'pop', 'jazz', 'classical', 'electronic', 'hip hop', 'blues', 'metal']\n",
    "TARGET_GENRES = [g for g in TARGET_GENRES if g in CONCEPTS.get('genre', [])]\n",
    "print(f\"Target genres: {TARGET_GENRES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf96090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_array: np.ndarray, sr: int, target_sr: int = 16000, duration: float = 3.0) -> torch.Tensor:\n",
    "    \"\"\"Preprocess audio to fixed length and sample rate.\"\"\"\n",
    "    # Resample if needed\n",
    "    if sr != target_sr:\n",
    "        audio_array = librosa.resample(audio_array, orig_sr=sr, target_sr=target_sr)\n",
    "    \n",
    "    # Trim or pad to fixed duration\n",
    "    target_length = int(target_sr * duration)\n",
    "    if len(audio_array) > target_length:\n",
    "        audio_array = audio_array[:target_length]\n",
    "    else:\n",
    "        audio_array = np.pad(audio_array, (0, target_length - len(audio_array)))\n",
    "    \n",
    "    return torch.from_numpy(audio_array).float()\n",
    "\n",
    "# Load and preprocess all GTZAN audio\n",
    "print(\"Loading and preprocessing GTZAN audio files...\")\n",
    "X_train, y_train = [], []\n",
    "\n",
    "for file_info in tqdm(audio_files, desc=\"Loading audio\"):\n",
    "    # Load audio using librosa\n",
    "    audio, sr = librosa.load(file_info['path'], sr=None, mono=True)\n",
    "    \n",
    "    # Preprocess audio\n",
    "    audio_tensor = preprocess_audio(audio, sr)\n",
    "    X_train.append(audio_tensor)\n",
    "    y_train.append(file_info['label'])\n",
    "\n",
    "X_train = torch.stack(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Labels shape: {y_train.shape}\")\n",
    "print(f\"Sample rate: 16000 Hz, Duration: 3.0 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GTZAN dataset from local folder\n",
    "print(\"Loading GTZAN dataset from local files...\")\n",
    "GTZAN_PATH = Path(\"../outputs/gtzan/Data/genres_original\")\n",
    "\n",
    "# Map genre names\n",
    "GENRE_MAP = {\n",
    "    'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4,\n",
    "    'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock': 9\n",
    "}\n",
    "TARGET_GENRES = list(GENRE_MAP.keys())\n",
    "\n",
    "# Load all audio files\n",
    "print(\"Scanning audio files...\")\n",
    "audio_files = []\n",
    "for genre in TARGET_GENRES:\n",
    "    genre_path = GTZAN_PATH / genre\n",
    "    wav_files = sorted(genre_path.glob(\"*.wav\"))\n",
    "    for wav_file in wav_files:\n",
    "        audio_files.append({\n",
    "            'path': wav_file,\n",
    "            'genre': genre,\n",
    "            'label': GENRE_MAP[genre]\n",
    "        })\n",
    "\n",
    "print(f\"Loaded {len(audio_files)} audio files\")\n",
    "print(f\"Genres: {TARGET_GENRES}\")\n",
    "print(f\"Files per genre: ~{len(audio_files) // len(TARGET_GENRES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd2b9d",
   "metadata": {},
   "source": [
    "## 2. Load GTZAN Dataset and Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenreClassifier(torch.nn.Module):\n",
    "    \"\"\"Simple CNN-based music genre classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_genres: int = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=16000, n_fft=400, hop_length=160, n_mels=64\n",
    "        )\n",
    "        \n",
    "        # CNN feature extractor\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            \n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            \n",
    "            torch.nn.Conv2d(64, 128, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        # Bottleneck layer (for TCAV)\n",
    "        self.bottleneck = torch.nn.Linear(128, 256)\n",
    "        self.classifier = torch.nn.Linear(256, num_genres)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, return_bottleneck: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # Convert to mel-spectrogram\n",
    "        if x.shape[1] == 1 and x.shape[2] > 1000:\n",
    "            x = self.mel_spectrogram(x)\n",
    "            x = torchaudio.transforms.AmplitudeToDB()(x)\n",
    "        \n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        bottleneck = self.bottleneck(x)\n",
    "        x = self.relu(bottleneck)\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        if return_bottleneck:\n",
    "            return logits, bottleneck\n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = MusicGenreClassifier(num_genres=len(TARGET_GENRES))\n",
    "model = model.to(device)\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "print(\"Training genre classifier on GTZAN...\")\n",
    "\n",
    "# Split data\n",
    "indices = torch.randperm(len(X_train))\n",
    "train_size = int(0.8 * len(X_train))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "X_train_split = X_train[train_indices]\n",
    "y_train_split = y_train[train_indices]\n",
    "X_val = X_train[val_indices]\n",
    "y_val = y_train[val_indices]\n",
    "\n",
    "# Training setup\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(0, len(X_train_split), batch_size):\n",
    "        batch_X = X_train_split[i:i+batch_size].to(device)\n",
    "        batch_y = y_train_split[i:i+batch_size].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_X, return_bottleneck=False)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_val), batch_size):\n",
    "            batch_X = X_val[i:i+batch_size].to(device)\n",
    "            batch_y = y_val[i:i+batch_size].to(device)\n",
    "            logits = model(batch_X, return_bottleneck=False)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_total += batch_y.size(0)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss/len(X_train_split):.4f} - \"\n",
    "          f\"Train Acc: {100*correct/total:.2f}% - Val Acc: {100*val_correct/val_total:.2f}%\")\n",
    "\n",
    "model.eval()\n",
    "print(\"âœ“ Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12730c",
   "metadata": {},
   "source": [
    "## 3. Load Real Audio from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generated audio dataset from local files\n",
    "print(\"Loading generated audio dataset from local files...\")\n",
    "\n",
    "# Load metadata from per_sample_scores.csv\n",
    "AUDIO_DATA_PATH = Path(\"../outputs/tta/subset\")\n",
    "metadata_df = pd.read_csv(AUDIO_DATA_PATH / \"per_sample_scores.csv\")\n",
    "\n",
    "# Parse aspect_list column (it's stored as string representation of list)\n",
    "import ast\n",
    "metadata_df['aspect_list_parsed'] = metadata_df['aspect_list'].apply(\n",
    "    lambda x: ast.literal_eval(x) if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(metadata_df)} audio samples\")\n",
    "print(f\"Audio files location: {AUDIO_DATA_PATH / 'audio_samples'}\")\n",
    "print(f\"\\nSample aspects: {metadata_df['aspect_list_parsed'].iloc[0][:5]}...\")\n",
    "\n",
    "def get_audio_by_tags(tags: List[str], num_samples: int) -> List[torch.Tensor]:\n",
    "    \"\"\"Load real audio samples that match given tags.\"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    # Filter by aspect list\n",
    "    matching_indices = []\n",
    "    for idx, row in metadata_df.iterrows():\n",
    "        aspect_list = row['aspect_list_parsed']\n",
    "        if any(tag in aspect_list for tag in tags):\n",
    "            matching_indices.append(idx)\n",
    "            if len(matching_indices) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Load audio files\n",
    "    for idx in matching_indices[:num_samples]:\n",
    "        filename = metadata_df.iloc[idx]['filename']\n",
    "        audio_path = AUDIO_DATA_PATH / 'audio_samples' / filename\n",
    "        \n",
    "        if audio_path.exists():\n",
    "            audio, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "            audio_tensor = preprocess_audio(audio, sr)\n",
    "            samples.append(audio_tensor)\n",
    "    \n",
    "    # If not enough samples, pad with random ones\n",
    "    while len(samples) < num_samples:\n",
    "        idx = np.random.randint(0, len(metadata_df))\n",
    "        filename = metadata_df.iloc[idx]['filename']\n",
    "        audio_path = AUDIO_DATA_PATH / 'audio_samples' / filename\n",
    "        \n",
    "        if audio_path.exists():\n",
    "            audio, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "            audio_tensor = preprocess_audio(audio, sr)\n",
    "            samples.append(audio_tensor)\n",
    "    \n",
    "    return samples[:num_samples]\n",
    "\n",
    "\n",
    "def create_concept_dataset_from_audio(concepts: List[str], num_samples: int = 20) -> torch.Tensor:\n",
    "    \"\"\"Create dataset from real audio matching concept tags.\"\"\"\n",
    "    samples = get_audio_by_tags(concepts, num_samples)\n",
    "    return torch.stack(samples)\n",
    "\n",
    "\n",
    "def create_random_audio_dataset(num_samples: int = 30) -> torch.Tensor:\n",
    "    \"\"\"Create random audio samples from dataset.\"\"\"\n",
    "    samples = []\n",
    "    indices = np.random.choice(len(metadata_df), min(num_samples, len(metadata_df)), replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        filename = metadata_df.iloc[idx]['filename']\n",
    "        audio_path = AUDIO_DATA_PATH / 'audio_samples' / filename\n",
    "        \n",
    "        if audio_path.exists():\n",
    "            audio, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "            audio_tensor = preprocess_audio(audio, sr)\n",
    "            samples.append(audio_tensor)\n",
    "    \n",
    "    return torch.stack(samples)\n",
    "\n",
    "print(\"Audio loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8b02f",
   "metadata": {},
   "source": [
    "## 4. TCAV Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25048a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCAV:\n",
    "    \"\"\"Testing with Concept Activation Vectors.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: torch.nn.Module, device: torch.device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "    \n",
    "    def get_activations(self, audio: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"Extract bottleneck activations.\"\"\"\n",
    "        audio = audio.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            _, bottleneck = self.model(audio)\n",
    "        return bottleneck.cpu().numpy()\n",
    "    \n",
    "    def train_cav(self, concept_acts: np.ndarray, random_acts: np.ndarray, \n",
    "                  num_runs: int = 5) -> Dict:\n",
    "        \"\"\"Train CAV (Concept Activation Vector) using logistic regression.\"\"\"\n",
    "        cavs, scores = [], []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            X = np.vstack([concept_acts, random_acts])\n",
    "            y = np.hstack([np.ones(len(concept_acts)), np.zeros(len(random_acts))])\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.3, random_state=run\n",
    "            )\n",
    "            \n",
    "            clf = LogisticRegression(max_iter=1000, random_state=run)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            cav = clf.coef_[0]\n",
    "            cav = cav / (np.linalg.norm(cav) + 1e-8)\n",
    "            \n",
    "            cavs.append(cav)\n",
    "            scores.append(accuracy_score(y_test, clf.predict(X_test)))\n",
    "        \n",
    "        return {\n",
    "            'cav': np.mean(cavs, axis=0),\n",
    "            'accuracy': np.mean(scores),\n",
    "            'accuracy_std': np.std(scores)\n",
    "        }\n",
    "    \n",
    "    def compute_tcav_score(self, activations: np.ndarray, cav: np.ndarray) -> float:\n",
    "        \"\"\"Compute TCAV score (fraction with positive sensitivity).\"\"\"\n",
    "        sensitivities = np.dot(activations, cav)\n",
    "        return np.mean(sensitivities > 0)\n",
    "\n",
    "tcav = TCAV(model, device)\n",
    "print(\"TCAV analyzer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25f080c",
   "metadata": {},
   "source": [
    "## 5. Run TCAV Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what aspects are actually present in the dataset\n",
    "all_aspects = set()\n",
    "for aspects in metadata_df['aspect_list_parsed']:\n",
    "    all_aspects.update(aspects)\n",
    "\n",
    "print(f\"Total unique aspects in dataset: {len(all_aspects)}\")\n",
    "print(f\"Sample aspects: {sorted(list(all_aspects))[:20]}\")\n",
    "\n",
    "# Define concepts to analyze based on what's in the dataset\n",
    "ANALYSIS_CONCEPTS = {\n",
    "    'tempo': ['fast tempo', 'slow tempo', 'medium tempo', 'uptempo'],\n",
    "    'instrument': ['guitar', 'piano', 'drums', 'e-guitar', 'acoustic guitar', 'bass', 'e-bass'],\n",
    "    'mood': ['emotional', 'energetic', 'passionate', 'soulful', 'aggressive', 'ambient'],\n",
    "    'style': ['rock', 'pop', 'blues', 'folk', 'techno']\n",
    "}\n",
    "\n",
    "# Filter to concepts that exist in our dataset\n",
    "for cat in list(ANALYSIS_CONCEPTS.keys()):\n",
    "    existing_concepts = [c for c in ANALYSIS_CONCEPTS[cat] if c in all_aspects]\n",
    "    if existing_concepts:\n",
    "        ANALYSIS_CONCEPTS[cat] = existing_concepts\n",
    "    else:\n",
    "        del ANALYSIS_CONCEPTS[cat]\n",
    "\n",
    "# Concept mapping for variations\n",
    "CONCEPT_TO_VARIANTS = {\n",
    "    'guitar': ['guitar', 'acoustic guitar', 'e-guitar'],\n",
    "    'piano': ['piano', 'keyboard'],\n",
    "    'drums': ['drums', 'percussion', 'acoustic drums', 'digital drums', 'electronic drums'],\n",
    "    'bass': ['bass', 'e-bass'],\n",
    "    'energetic': ['energetic', 'upbeat'],\n",
    "    'emotional': ['emotional', 'passionate', 'soulful'],\n",
    "    'fast tempo': ['fast tempo', 'uptempo'],\n",
    "    'slow tempo': ['slow tempo', 'medium tempo']\n",
    "}\n",
    "\n",
    "print(\"\\nConcepts for analysis:\")\n",
    "for cat, concepts in ANALYSIS_CONCEPTS.items():\n",
    "    print(f\"  {cat}: {concepts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434bf92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random counterexamples from real audio\n",
    "random_data = create_random_audio_dataset(num_samples=50)\n",
    "random_acts = tcav.get_activations(random_data)\n",
    "\n",
    "# Run TCAV analysis\n",
    "results = defaultdict(dict)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TCAV ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for category, concept_list in ANALYSIS_CONCEPTS.items():\n",
    "    print(f\"\\nðŸ“Š Category: {category.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for concept in concept_list:\n",
    "        # Get concept variants\n",
    "        search_tags = CONCEPT_TO_VARIANTS.get(concept, [concept])\n",
    "        \n",
    "        # Load real audio for concept\n",
    "        print(f\"  Loading audio for '{concept}'...\")\n",
    "        concept_data = create_concept_dataset_from_audio(search_tags, num_samples=30)\n",
    "        concept_acts = tcav.get_activations(concept_data)\n",
    "        \n",
    "        # Train CAV\n",
    "        cav_result = tcav.train_cav(concept_acts, random_acts, num_runs=5)\n",
    "        \n",
    "        # Compute TCAV scores for each genre using GTZAN samples\n",
    "        genre_scores = {}\n",
    "        for genre_name in TARGET_GENRES:\n",
    "            # Get GTZAN samples for this genre\n",
    "            genre_idx = GENRE_MAP[genre_name]\n",
    "            genre_mask = y_train == genre_idx\n",
    "            genre_samples = X_train[genre_mask][:20]  # Take first 20 samples\n",
    "            \n",
    "            genre_acts = tcav.get_activations(genre_samples)\n",
    "            genre_scores[genre_name] = tcav.compute_tcav_score(genre_acts, cav_result['cav'])\n",
    "        \n",
    "        results[category][concept] = {\n",
    "            'cav_accuracy': cav_result['accuracy'],\n",
    "            'genre_scores': genre_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"  {concept}: CAV acc={cav_result['accuracy']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ Analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d5330",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TCAV scores heatmap\n",
    "all_concepts = [c for concepts in ANALYSIS_CONCEPTS.values() for c in concepts]\n",
    "\n",
    "# Build score matrix\n",
    "score_matrix = []\n",
    "concept_names = []\n",
    "\n",
    "for category, concept_dict in results.items():\n",
    "    for concept, data in concept_dict.items():\n",
    "        scores = [data['genre_scores'].get(g, 0.5) for g in TARGET_GENRES]\n",
    "        score_matrix.append(scores)\n",
    "        concept_names.append(f\"{concept}\\n({category})\")\n",
    "\n",
    "score_matrix = np.array(score_matrix)\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    score_matrix, \n",
    "    xticklabels=TARGET_GENRES,\n",
    "    yticklabels=concept_names,\n",
    "    annot=True, \n",
    "    fmt='.2f',\n",
    "    cmap='RdYlGn',\n",
    "    vmin=0, vmax=1,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('TCAV Scores: Concept Importance per Genre', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Genre')\n",
    "ax.set_ylabel('Concept')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CAV classifier accuracies\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "accuracies = []\n",
    "labels = []\n",
    "colors = []\n",
    "color_map = {'tempo': 'steelblue', 'instrument': 'coral', 'mood': 'forestgreen'}\n",
    "\n",
    "for category, concept_dict in results.items():\n",
    "    for concept, data in concept_dict.items():\n",
    "        accuracies.append(data['cav_accuracy'])\n",
    "        labels.append(concept)\n",
    "        colors.append(color_map.get(category, 'gray'))\n",
    "\n",
    "bars = ax.bar(range(len(accuracies)), accuracies, color=colors, alpha=0.8)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.set_ylabel('CAV Classifier Accuracy')\n",
    "ax.set_title('CAV Training Accuracy per Concept', fontweight='bold')\n",
    "ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add legend for categories\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, label=cat) for cat, c in color_map.items()]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd7e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for category, concept_dict in results.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for concept, data in concept_dict.items():\n",
    "        scores = list(data['genre_scores'].values())\n",
    "        print(f\"  {concept}:\")\n",
    "        print(f\"    CAV Accuracy: {data['cav_accuracy']:.3f}\")\n",
    "        print(f\"    TCAV Score Range: [{min(scores):.3f}, {max(scores):.3f}]\")\n",
    "        print(f\"    Mean TCAV Score: {np.mean(scores):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
