{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "508e477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad965595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.tcav.model import MusicGenreClassifier\n",
    "from src.tcav.tcav import TCAV\n",
    "from src.constants import GTZAN_PATH, METADATA_CSV_PATH, AUDIO_DATA_PATH, TCAV_RESULTS_PATH, GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (18, 8),\n",
    "    'font.size': 32,\n",
    "    'axes.titlesize': 28,\n",
    "    'axes.labelsize': 25,\n",
    "    'xtick.labelsize': 23,\n",
    "    'ytick.labelsize': 23,\n",
    "    'legend.fontsize': 21\n",
    "})\n",
    "\n",
    "TRAIN_MODEL = False\n",
    "RUN_TCAV_ANALYSIS = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d9ce9",
   "metadata": {},
   "source": [
    "## Load Custom Concept Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda61786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available concept categories:\n",
      "  tempo: 50 tags (e.g., ['medium tempo', 'slow tempo', 'fast tempo'])\n",
      "  genre: 50 tags (e.g., ['rock', 'pop', 'electronic music'])\n",
      "  mood: 50 tags (e.g., ['emotional', 'passionate', 'energetic'])\n",
      "  instrument: 50 tags (e.g., ['acoustic drums', 'electric guitar', 'bass guitar'])\n"
     ]
    }
   ],
   "source": [
    "# Load concept-to-tags mapping\n",
    "CONCEPTS = json.load(open(\"../data/concepts_to_tags.json\", \"r\"))\n",
    "\n",
    "print(\"Available concept categories:\")\n",
    "for cat, tags in CONCEPTS.items():\n",
    "    print(f\"  {cat}: {len(tags)} tags (e.g., {tags[:3]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55baceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse mapping\n",
    "TAG_TO_CATEGORY = {}\n",
    "for cat, tags in CONCEPTS.items():\n",
    "    for tag in tags:\n",
    "        TAG_TO_CATEGORY[tag] = cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf96090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_array: np.ndarray, sr: int, target_sr: int = 16000, duration: float = 3.0) -> torch.Tensor:\n",
    "    \"\"\"Preprocess audio to fixed length and sample rate.\"\"\"\n",
    "    if sr != target_sr:\n",
    "        audio_array = librosa.resample(audio_array, orig_sr=sr, target_sr=target_sr)\n",
    "    \n",
    "    target_length = int(target_sr * duration)\n",
    "    if len(audio_array) > target_length:\n",
    "        audio_array = audio_array[:target_length]\n",
    "    else:\n",
    "        audio_array = np.pad(audio_array, (0, target_length - len(audio_array)))\n",
    "    \n",
    "    return torch.from_numpy(audio_array).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71cb2090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GTZAN dataset from local files...\n",
      "Scanning audio files...\n",
      "Loaded 1000 audio files\n",
      "Genres: ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
      "Files per genre: ~100\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading GTZAN dataset from local files...\")\n",
    "\n",
    "GENRE_MAP = {\n",
    "    'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4,\n",
    "    'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock': 9\n",
    "}\n",
    "TARGET_GENRES = list(GENRE_MAP.keys())\n",
    "\n",
    "print(\"Scanning audio files...\")\n",
    "audio_files = []\n",
    "for genre in TARGET_GENRES:\n",
    "    genre_path = GTZAN_PATH / genre\n",
    "    wav_files = sorted(genre_path.glob(\"*.wav\"))\n",
    "    for wav_file in wav_files:\n",
    "        audio_files.append({\n",
    "            'path': wav_file,\n",
    "            'genre': genre,\n",
    "            'label': GENRE_MAP[genre]\n",
    "        })\n",
    "\n",
    "print(f\"Loaded {len(audio_files)} audio files\")\n",
    "print(f\"Genres: {TARGET_GENRES}\")\n",
    "print(f\"Files per genre: ~{len(audio_files) // len(TARGET_GENRES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380e798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing GTZAN audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading audio:  55%|█████▍    | 549/1000 [00:17<00:06, 67.16it/s]/tmp/ipykernel_44372/751458001.py:6: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(file_info['path'], sr=None, mono=True)\n",
      "/home/bruno/miniconda3/envs/concept-caps/lib/python3.12/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Loading audio:  56%|█████▋    | 564/1000 [00:18<00:06, 65.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading /media/bruno/B438-3BD6/datasets/GTZAN/Data/genres_original/jazz/jazz.00054.wav: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading audio: 100%|██████████| 1000/1000 [00:23<00:00, 42.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([999, 48000])\n",
      "Labels shape: torch.Size([999])\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preprocessing GTZAN audio files...\")\n",
    "X_train, y_train = [], []\n",
    "\n",
    "for file_info in tqdm(audio_files, desc=\"Loading audio\"):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_info['path'], sr=None, mono=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_info['path']}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    audio_tensor = preprocess_audio(audio, sr)\n",
    "    X_train.append(audio_tensor)\n",
    "    y_train.append(file_info['label'])\n",
    "\n",
    "X_train = torch.stack(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Labels shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd2b9d",
   "metadata": {},
   "source": [
    "## Load GTZAN Dataset and Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e4e6e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 128,714\n"
     ]
    }
   ],
   "source": [
    "model = MusicGenreClassifier(num_genres=len(TARGET_GENRES))\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5acc4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mode: DISABLED\n",
      "Model checkpoint path: ../models/best-genre-classifier.ckpt\n"
     ]
    }
   ],
   "source": [
    "GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Training mode: {'ENABLED' if TRAIN_MODEL else 'DISABLED'}\")\n",
    "print(f\"Model checkpoint path: {GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "807c3082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model from checkpoint...\n",
      "Model loaded from: ../models/best-genre-classifier.ckpt\n",
      "Model ready on cuda\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    print(\"Training genre classifier on GTZAN...\")\n",
    "    \n",
    "    indices = torch.randperm(len(X_train))\n",
    "    train_size = int(0.8 * len(X_train))\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    X_train_split = X_train[train_indices]\n",
    "    y_train_split = y_train[train_indices]\n",
    "    X_val = X_train[val_indices]\n",
    "    y_val = y_train[val_indices]\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_split, y_train_split)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        dirpath=GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH.parent,\n",
    "        filename='genre_classifier_best',\n",
    "        monitor='val_acc',\n",
    "        mode='max',\n",
    "        save_top_k=1,\n",
    "        save_last=True\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            pl.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=20,\n",
    "                mode='min'\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    print(f\"Model training complete!\")\n",
    "    print(f\"Best model saved to: {checkpoint_callback.best_model_path}\")\n",
    "    \n",
    "    model = MusicGenreClassifier.load_from_checkpoint(\n",
    "        checkpoint_callback.best_model_path,\n",
    "        num_genres=len(TARGET_GENRES)\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"Loading pre-trained model from checkpoint...\")\n",
    "    \n",
    "    if GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH.exists():\n",
    "        model = MusicGenreClassifier.load_from_checkpoint(\n",
    "            GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH,\n",
    "            num_genres=len(TARGET_GENRES)\n",
    "        )\n",
    "        print(f\"Model loaded from: {GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH}\")\n",
    "    else:\n",
    "        print(f\"Checkpoint not found at {GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH}\")\n",
    "        print(\"Please set TRAIN_MODEL=True to train a new model first.\")\n",
    "        raise FileNotFoundError(f\"Model checkpoint not found: {GENRE_CLASSIFIER_MODEL_CHECKPOINT_PATH}\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model ready on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd4eb6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model accuracy on GTZAN validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 63/63 [00:00<00:00, 122.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on GTZAN: 86.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model accuracy on GTZAN validation set...\")\n",
    "val_dataset = TensorDataset(X_train, y_train)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs, return_bottleneck=False)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy on GTZAN: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12730c",
   "metadata": {},
   "source": [
    "## Load Real Audio from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading generated audio dataset from local files...\")\n",
    "\n",
    "metadata_df = pd.read_csv(METADATA_CSV_PATH)\n",
    "metadata_df['aspect_list_parsed'] = metadata_df['aspect_list'].apply(\n",
    "    lambda x: ast.literal_eval(x) if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(metadata_df)} audio samples\")\n",
    "print(f\"Audio files location: {AUDIO_DATA_PATH}\")\n",
    "print(f\"\\nSample aspects: {metadata_df['aspect_list_parsed'].iloc[0][:5]}...\")\n",
    "\n",
    "def get_audio_by_tags(tag: str, num_samples: int) -> List[torch.Tensor]:\n",
    "    \"\"\"Load real audio samples that match given tags.\"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    matching_indices = []\n",
    "    for idx, row in metadata_df.iterrows():\n",
    "        aspect_list = row['aspect_list_parsed']\n",
    "        if tag in aspect_list:\n",
    "            matching_indices.append(idx)\n",
    "            if len(matching_indices) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    loaded_count = 0\n",
    "    for idx in matching_indices:\n",
    "        filename = metadata_df.iloc[idx]['filename']\n",
    "        audio_path = AUDIO_DATA_PATH / filename\n",
    "        \n",
    "        if audio_path.exists():\n",
    "            try:\n",
    "                audio, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "                audio_tensor = preprocess_audio(audio, sr)\n",
    "                samples.append(audio_tensor)\n",
    "                loaded_count += 1\n",
    "                if loaded_count >= num_samples:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {audio_path}: {e}\")\n",
    "    \n",
    "    if num_samples > len(samples):\n",
    "        print(f\"Skipping concept {tag}: Found {len(samples)}/{num_samples} samples.\")\n",
    "        return None\n",
    "    \n",
    "    return samples[:num_samples]\n",
    "\n",
    "\n",
    "def create_concept_dataset_from_audio(concept: str, num_samples: int = 20) -> torch.Tensor:\n",
    "    \"\"\"Create dataset from real audio matching concept tags.\"\"\"\n",
    "    samples = get_audio_by_tags(concept, num_samples)\n",
    "    if samples is None:\n",
    "        return None\n",
    "    return torch.stack(samples)\n",
    "\n",
    "\n",
    "def create_random_audio_dataset(num_samples: int = 30) -> torch.Tensor:\n",
    "    \"\"\"Create random audio samples from dataset.\"\"\"\n",
    "    samples = []\n",
    "    indices = np.random.choice(len(metadata_df), min(num_samples, len(metadata_df)), replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        filename = metadata_df.iloc[idx]['filename']\n",
    "        audio_path = AUDIO_DATA_PATH / filename\n",
    "        \n",
    "        if audio_path.exists():\n",
    "            audio, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "            audio_tensor = preprocess_audio(audio, sr)\n",
    "            samples.append(audio_tensor)\n",
    "    \n",
    "    return torch.stack(samples)\n",
    "\n",
    "print(\"Audio loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25f080c",
   "metadata": {},
   "source": [
    "## Run TCAV Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25048a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcav = TCAV(model, device)\n",
    "print(\"TCAV analyzer initialized with improved implementation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aspects = set()\n",
    "for aspects in metadata_df['aspect_list_parsed']:\n",
    "    all_aspects.update(aspects)\n",
    "\n",
    "print(f\"Total unique aspects in dataset: {len(all_aspects)}\")\n",
    "print(f\"Sample aspects: {sorted(list(all_aspects))[:20]}\")\n",
    "\n",
    "ANALYSIS_CONCEPTS = {\n",
    "    'tempo': CONCEPTS.get('tempo'),\n",
    "    'instrument': CONCEPTS.get('instrument'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434bf92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RUN_TCAV_ANALYSIS:\n",
    "    random_data = create_random_audio_dataset(num_samples=100)\n",
    "    random_acts = tcav.get_activations(random_data)\n",
    "\n",
    "    results = defaultdict(dict)\n",
    "    for category, concept_list in ANALYSIS_CONCEPTS.items():\n",
    "        print(f\"Category: {category.upper()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for concept in concept_list[:10]:\n",
    "            print(f\"  Loading audio for '{concept}'...\")\n",
    "            concept_data = create_concept_dataset_from_audio(concept, num_samples=50)\n",
    "\n",
    "            if concept_data is None:\n",
    "                print(f\"  Skipping concept '{concept}' due to insufficient samples.\")\n",
    "                continue\n",
    "\n",
    "            concept_acts = tcav.get_activations(concept_data)\n",
    "            \n",
    "            cav_result = tcav.train_cav(concept_acts, random_acts, num_runs=40)\n",
    "\n",
    "            if cav_result['cav'] is None:\n",
    "                print(f\"  Skipping concept '{concept}' due to low CAV accuracy ({cav_result['accuracy']:.3f}).\")\n",
    "                continue\n",
    "            \n",
    "            genre_scores = {}\n",
    "            for genre_name in TARGET_GENRES:\n",
    "                genre_idx = GENRE_MAP[genre_name]\n",
    "                genre_mask = y_train == genre_idx\n",
    "                genre_samples = X_train[genre_mask]\n",
    "                \n",
    "                genre_acts = tcav.get_activations(genre_samples)\n",
    "                genre_scores[genre_name] = tcav.compute_tcav_score(genre_acts, cav_result['cav'], method='cosine')\n",
    "            \n",
    "            results[category][concept] = {\n",
    "                'cav_accuracy': cav_result['accuracy'],\n",
    "                'genre_scores': genre_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"  {concept}: CAV acc={cav_result['accuracy']:.3f}\")\n",
    "\n",
    "    with open(TCAV_RESULTS_PATH, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"TCAV results saved to: {TCAV_RESULTS_PATH}\")\n",
    "else:\n",
    "    with open(TCAV_RESULTS_PATH, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"TCAV results loaded from: {TCAV_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d5330",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_concepts = [c for concepts in ANALYSIS_CONCEPTS.values() for c in concepts]\n",
    "\n",
    "score_matrix = []\n",
    "concept_names = []\n",
    "\n",
    "for category, concept_dict in results.items():\n",
    "    for concept, data in concept_dict.items():\n",
    "        scores = [data['genre_scores'].get(g, 0.0) for g in TARGET_GENRES]\n",
    "        score_matrix.append(scores)\n",
    "        concept_names.append(f\"{concept}\\n({category})\")\n",
    "\n",
    "score_matrix = np.array(score_matrix)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(22, 22))\n",
    "sns.heatmap(\n",
    "    score_matrix, \n",
    "    xticklabels=TARGET_GENRES,\n",
    "    yticklabels=concept_names,\n",
    "    annot=True, \n",
    "    fmt='.2f',\n",
    "    vmin=0, vmax=1,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title('TCAV Scores: Concept Importance per Genre', fontweight='bold')\n",
    "ax.set_xlabel('Genre')\n",
    "ax.set_ylabel('Concept')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f10ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 14))\n",
    "\n",
    "concept_names = []\n",
    "y_positions = []\n",
    "y_pos = 0\n",
    "\n",
    "for category, concept_dict in results.items():\n",
    "    for concept, data in concept_dict.items():\n",
    "        for i, (genre, score) in enumerate(data['genre_scores'].items()):\n",
    "            size = score * 500  # Scale for visibility\n",
    "            color = plt.cm.RdYlGn(score)  # Red to Green colormap\n",
    "            \n",
    "            ax.scatter(i, y_pos, s=size, c=[color], alpha=0.8, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        concept_names.append(f\"{concept} ({category})\")\n",
    "        y_positions.append(y_pos)\n",
    "        y_pos += 1\n",
    "\n",
    "ax.set_xticks(range(len(TARGET_GENRES)))\n",
    "ax.set_xticklabels(TARGET_GENRES, rotation=45, ha='right')\n",
    "ax.set_yticks(y_positions)\n",
    "ax.set_yticklabels(concept_names)\n",
    "ax.set_xlabel('Genre', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Concept', fontsize=12, fontweight='bold')\n",
    "ax.set_title('TCAV Scores: Concept Importance per Genre\\n(size & color indicate strength)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "ax.set_xlim(-0.5, len(TARGET_GENRES)-0.5)\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlGn, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax, pad=0.02)\n",
    "cbar.set_label('TCAV Score', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "PLOT_GENRES = ['blues', 'country', 'jazz', 'pop', 'reggae', 'metal', 'rock']\n",
    "\n",
    "PLOT_CONCEPTS = {\n",
    "    'tempo': ['slow tempo', 'moderate tempo', 'fast tempo'],\n",
    "    'instrument': ['bass guitar', 'piano', 'acoustic guitar', 'punchy kick']\n",
    "}\n",
    "\n",
    "concept_labels = []\n",
    "all_scores_by_concept = []\n",
    "all_categories = []\n",
    "\n",
    "for category, concept_dict in results.items():\n",
    "    if category not in PLOT_CONCEPTS:\n",
    "        continue\n",
    "    \n",
    "    for concept, data in concept_dict.items():\n",
    "        if concept not in PLOT_CONCEPTS[category]:\n",
    "            continue\n",
    "        \n",
    "        concept_labels.append(concept)\n",
    "        all_categories.append(category)\n",
    "        scores = [data['genre_scores'].get(g, 0.0) for g in PLOT_GENRES]\n",
    "        all_scores_by_concept.append(scores)\n",
    "\n",
    "mean_scores = [np.mean(scores) for scores in all_scores_by_concept]\n",
    "sorted_indices = np.argsort(mean_scores)[::-1]  # Sort descending\n",
    "\n",
    "concept_labels_sorted = [concept_labels[i] for i in sorted_indices]\n",
    "all_categories_sorted = [all_categories[i] for i in sorted_indices]\n",
    "all_scores_sorted = [all_scores_by_concept[i] for i in sorted_indices]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 7))\n",
    "x = np.arange(len(concept_labels_sorted))\n",
    "width = 0.12\n",
    "\n",
    "for concept_idx in range(len(concept_labels_sorted)):\n",
    "    concept_scores = all_scores_sorted[concept_idx]\n",
    "    \n",
    "    genre_score_pairs = [(PLOT_GENRES[i], concept_scores[i], i) for i in range(len(PLOT_GENRES))]\n",
    "    genre_score_pairs.sort(key=lambda x: x[1])\n",
    "    \n",
    "    for bar_idx, (genre_name, score, original_idx) in enumerate(genre_score_pairs):\n",
    "        num_genres = len(PLOT_GENRES)\n",
    "        position = x[concept_idx] + (bar_idx - num_genres/2) * width + width/2\n",
    "        \n",
    "        bar = ax.bar(position, score, width, \n",
    "                    color='coral', edgecolor='black', \n",
    "                    linewidth=1.2, alpha=0.7)\n",
    "        \n",
    "        ax.text(position, 0.02, genre_name.upper(), \n",
    "               rotation=90, ha='center', va='bottom', \n",
    "               fontsize=16, fontweight='bold',\n",
    "               color='black')\n",
    "\n",
    "ax.set_xlabel('', fontweight='bold')\n",
    "ax.set_ylabel('TCAV Score', fontweight='bold')\n",
    "ax.set_title('Concept Importance Across Genres\\n(Higher → more important)', \n",
    "             fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([c for c in concept_labels_sorted], \n",
    "                   rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.savefig(\"../docs/assets/concept_importance_across_genres.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07079017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "PLOT_GENRES = ['blues', 'country', 'jazz', 'pop', 'reggae', 'metal', 'rock']\n",
    "\n",
    "PLOT_CONCEPTS = {\n",
    "    'tempo': ['slow tempo', 'moderate tempo', 'fast tempo'],\n",
    "    'instrument': ['bass guitar', 'acoustic guitar', 'punchy kick']\n",
    "}\n",
    "\n",
    "all_plot_concepts = [c for cat_list in PLOT_CONCEPTS.values() for c in cat_list]\n",
    "unique_concepts = sorted(list(set(all_plot_concepts)))\n",
    "petroff_colors = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "color_map = {concept: color for concept, color in zip(unique_concepts, petroff_colors)}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bar_width = 0.15\n",
    "x_indices = np.arange(len(PLOT_GENRES))\n",
    "\n",
    "for i, genre in enumerate(PLOT_GENRES):\n",
    "    genre_data = []\n",
    "    \n",
    "    for category, concept_dict in results.items():\n",
    "        if category not in PLOT_CONCEPTS: continue\n",
    "        for concept, data in concept_dict.items():\n",
    "            if concept not in PLOT_CONCEPTS[category]: continue\n",
    "            \n",
    "            score = data['genre_scores'].get(genre, 0.0)\n",
    "            genre_data.append((concept, score))\n",
    "    \n",
    "    genre_data.sort(key=lambda x: x[1])\n",
    "    n_bars = len(genre_data)\n",
    "    total_group_width = n_bars * bar_width\n",
    "    start_x = i - (total_group_width / 2) + (bar_width / 2)\n",
    "    \n",
    "    for j, (concept, score) in enumerate(genre_data):\n",
    "        pos = start_x + (j * bar_width)\n",
    "        ax.bar(pos, score, width=bar_width, edgecolor='black', linewidth=1.2, color=color_map[concept], alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x_indices)\n",
    "ax.set_xticklabels([g.upper() for g in PLOT_GENRES], fontweight='bold')\n",
    "ax.set_ylabel('TCAV Score', fontweight='bold')\n",
    "ax.set_title('Concept Influence on Genres\\n(Higher → more important)', fontweight='bold', pad=20)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=color_map[c], label=c) for c in unique_concepts]\n",
    "ax.legend(handles=legend_patches, \n",
    "          title='Concepts', \n",
    "          bbox_to_anchor=(1.01, 1), \n",
    "          loc='upper left',\n",
    "          frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../docs/assets/concept_influence_across_genres.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "accuracies = []\n",
    "labels = []\n",
    "colors = []\n",
    "color_map = {'tempo': 'steelblue', 'instrument': 'coral', 'mood': 'forestgreen'}\n",
    "\n",
    "for category, concept_dict in results.items():\n",
    "    for concept, data in concept_dict.items():\n",
    "        accuracies.append(data['cav_accuracy'])\n",
    "        labels.append(concept)\n",
    "        colors.append(color_map.get(category, 'gray'))\n",
    "\n",
    "bars = ax.bar(range(len(accuracies)), accuracies, color=colors, alpha=0.8)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.set_ylabel('CAV Classifier Accuracy')\n",
    "ax.set_title('CAV Training Accuracy per Concept', fontweight='bold')\n",
    "ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, label=cat) for cat, c in color_map.items()]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concept-caps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
